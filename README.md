# ISLBBNN
Make Neural Networks more interpretable by using Input-Skip Latent Binary Bayesian Neural Networks (ISLBBNN). Allows for global explanation of predictions, and can also be used for local explanations.

To see different implementations, look into the the `example` folder. Local explanations are used in the end of all LRT notebooks.  

Thesis related to this repository can be found at the following link: https://nmbu.brage.unit.no/nmbu-xmlui/bitstream/handle/11250/3147960/no.nmbu%3awiseflow%3a7110451%3a59111978.pdf?sequence=1&isAllowed=y

Alternatively, you can find it at https://nmbu.brage.unit.no/ by searching: Improving sparsity and interpretability of latent binary Bayesian neural networks by introducing input-skip connections.
