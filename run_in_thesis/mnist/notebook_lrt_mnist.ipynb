{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e650d1c0-23e5-4e02-b9ee-0af0103ff042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm, trange\n",
    "import pipeline_functions as pip_func\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append('layers')\n",
    "from config import config\n",
    "from lrt_layers import BayesianLinear\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# cuda = torch.cuda.set_device(0)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "    \n",
    "    \n",
    "# define the parameters\n",
    "BATCH_SIZE = 1500\n",
    "TEST_BATCH_SIZE = 1000\n",
    "CLASSES = 10\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "\n",
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "lr = config['lr']\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "# patience = config['patience']\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './mnist', train=True, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **LOADER_KWARGS)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './mnist', train=False, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=TEST_BATCH_SIZE, shuffle=False, **LOADER_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bbc4be-cd2e-4a66-8bf1-809aa558ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "Tot weights in model: 1314640\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:47<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 448023.90625\n",
      "nll 1088.798828125\n",
      "density 0.9992283289513628\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:49<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 440921.03125\n",
      "nll 646.8707885742188\n",
      "density 0.9990882970442577\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:48<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 434085.03125\n",
      "nll 500.3792724609375\n",
      "density 0.9988951969229155\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:48<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 427067.875\n",
      "nll 212.81211853027344\n",
      "density 0.9986259105167197\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:48<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 420207.90625\n",
      "nll 141.0936737060547\n",
      "density 0.9982424879543992\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:49<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 413300.875\n",
      "nll 110.03453063964844\n",
      "density 0.9976838703816616\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:48<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 406265.0\n",
      "nll 84.27394104003906\n",
      "density 0.9968498697344468\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:48<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 399005.90625\n",
      "nll 42.86791229248047\n",
      "density 0.995575852547178\n",
      "median weights tensor(1314640)\n",
      "\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:16<01:34,  2.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[0;32m--> 134\u001b[0m     nll, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     all_nll\u001b[38;5;241m.\u001b[39mappend(nll)\n\u001b[1;32m    136\u001b[0m     all_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     48\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# negative_log_likelihood = net.loss(outputs, target)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m negative_log_likelihood \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(outputs, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/skip_con/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/skip_con/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mBayesianNetwork.forward\u001b[0;34m(self, x, sample, ensemble)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ensemble\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     26\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, p)\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinears\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/skip_con/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/skip_con/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/fs-2/scale/OrionStore/Home/eirihoyh/mnist/layers/lrt_layers.py:58\u001b[0m, in \u001b[0;36mBayesianLinear.forward\u001b[0;34m(self, input, ensemble, sample, calculate_log_probs)\u001b[0m\n\u001b[1;32m     56\u001b[0m e_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m     57\u001b[0m var_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_sigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m e_b \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_mu\n\u001b[1;32m     59\u001b[0m var_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, var_w\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_sigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     60\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(var_b\u001b[38;5;241m.\u001b[39msize()), device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(train_loader.dataset)\n",
    "TEST_SIZE = len(test_loader.dataset)\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_TEST_BATCHES = len(test_loader)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0\n",
    "\n",
    "p = 28*28\n",
    "\n",
    "\n",
    "\n",
    "#-------SKIP CONNECTION LBBNN--------\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # set the architecture\n",
    "        self.linears = nn.ModuleList([BayesianLinear(p, dim, a_prior=0.1)])\n",
    "        self.linears.extend([BayesianLinear((dim+p), (dim), a_prior=0.1) for _ in range(HIDDEN_LAYERS-1)])\n",
    "        self.linears.append(BayesianLinear((dim+p), CLASSES, a_prior=0.1))\n",
    "        self.loss = nn.BCELoss(reduction='sum')  # Setup loss (Binary cross entropy as binary classification)\n",
    "        \n",
    "\n",
    "    def forward(self, x, sample=False, ensemble=False):\n",
    "        x_input = x.view(-1, p)\n",
    "        x = F.sigmoid(self.linears[0](x_input, ensemble))\n",
    "        i = 1\n",
    "        for l in self.linears[1:-1]:\n",
    "            x = F.sigmoid(l(torch.cat((x, x_input),1), ensemble))\n",
    "            i += 1\n",
    "\n",
    "        out = F.log_softmax((self.linears[i](torch.cat((x, x_input),1), ensemble)), dim=1)\n",
    "        return out\n",
    "\n",
    "    def kl(self):\n",
    "        kl_sum = self.linears[0].kl\n",
    "        for l in self.linears[1:]:\n",
    "            kl_sum = kl_sum + l.kl\n",
    "        return kl_sum\n",
    "    \n",
    "\n",
    "# Stochastic Variational Inference iteration\n",
    "def train(net, optimizer, epoch):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        net.zero_grad()\n",
    "        outputs = net(data, ensemble=True)\n",
    "        # negative_log_likelihood = net.loss(outputs, target)\n",
    "        negative_log_likelihood = F.nll_loss(outputs, target, reduction=\"sum\")\n",
    "        #if epoch <= 1000:\n",
    "        #    loss = negative_log_likelihood + (epoch/1000)*(net.kl() / NUM_BATCHES)\n",
    "        #else:\n",
    "        loss = negative_log_likelihood + net.kl() / NUM_BATCHES\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if verbose:\n",
    "        alpha_clean = pip_func.clean_alpha(net, threshold=0.5)\n",
    "        density_median, used_weigths_median, _ = pip_func.network_density_reduction(alpha_clean)\n",
    "        print('loss', loss.item())\n",
    "        print('nll', negative_log_likelihood.item())\n",
    "        print('density', pip_func.expected_number_of_weights(net, p)/nr_weights)\n",
    "        print(\"median weights\", used_weigths_median)\n",
    "        print('')\n",
    "    return negative_log_likelihood.item(), loss.item()\n",
    "\n",
    "\n",
    "def test_ensemble(net):\n",
    "    net.eval()\n",
    "    metr = []\n",
    "    ensemble = []\n",
    "    median = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            outputs = torch.zeros(TEST_SAMPLES, TEST_BATCH_SIZE, CLASSES).to(DEVICE)\n",
    "            out2 = torch.zeros_like(outputs)\n",
    "            for i in range(TEST_SAMPLES):\n",
    "                outputs[i] = net.forward(data, sample=True, ensemble=True)  # model avg over structures and weights\n",
    "                out2[i] = net.forward(data, sample=True, ensemble=False)  # only model avg over weights where a > 0.5\n",
    "\n",
    "            output1 = outputs.mean(0)\n",
    "            out2 = out2.mean(0)\n",
    "\n",
    "            pred1 = output1.max(1, keepdim=True)[1]  # index of max log-probability\n",
    "            pred2 = out2.max(1, keepdim=True)[1]\n",
    "\n",
    "            a = pred2.eq(target.view_as(pred2)).sum().item()\n",
    "            b = pred1.eq(target.view_as(pred1)).sum().item()\n",
    "            median.append(a)\n",
    "            ensemble.append(b)\n",
    "    # estimate hte sparsity\n",
    "    alpha_clean = pip_func.clean_alpha(net, threshold=0.5)\n",
    "    density_median, used_weigths_median, _ = pip_func.network_density_reduction(alpha_clean)\n",
    "    # density.append(density_median)\n",
    "    # used_weights.append(used_weigths_median)\n",
    "    # g1 = ((net.l1.alpha_q.detach().cpu().numpy() > 0.5) * 1.)\n",
    "    # g2 = ((net.l2.alpha_q.detach().cpu().numpy() > 0.5) * 1.)\n",
    "    # g3 = ((net.l3.alpha_q.detach().cpu().numpy() > 0.5) * 1.)\n",
    "    # gs = np.concatenate((g1.flatten(), g2.flatten(), g3.flatten()))\n",
    "    metr.append(np.sum(median) / TEST_SIZE)\n",
    "    metr.append(np.sum(ensemble) / TEST_SIZE)\n",
    "    metr.append(density_median.cpu().detach().numpy())\n",
    "    metr.append(used_weigths_median.cpu().detach().numpy())\n",
    "    if verbose:\n",
    "        print(density_median, 'sparsity')\n",
    "        print(used_weigths_median, 'nr weights')\n",
    "        print(np.sum(median) / TEST_SIZE, 'median')\n",
    "        print(np.sum(ensemble) / TEST_SIZE, 'ensemble')\n",
    "    return metr\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "nll_several_runs = []\n",
    "loss_several_runs = []\n",
    "metrics_several_runs = []\n",
    "\n",
    "# make inference on 10 networks\n",
    "for i in range(n_nets):\n",
    "    print('network', i)\n",
    "    torch.manual_seed(i)\n",
    "    net = BayesianNetwork().to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(f'Tot weights in model: {nr_weights}')\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "    t1 = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch', epoch)\n",
    "        nll, loss = train(net, optimizer, epoch)\n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "    nll_several_runs.append(all_nll)\n",
    "    loss_several_runs.append(all_loss)\n",
    "    t = round((time.time() - t1), 1)\n",
    "    if save_res:\n",
    "        torch.save(net, f\"network/lrt_class/net{i}_sigmoid\")\n",
    "    metrics = test_ensemble(net)\n",
    "    metrics.append(t / epochs)\n",
    "    metrics_several_runs.append(metrics)\n",
    "    \n",
    "\n",
    "if save_res:\n",
    "    np.savetxt(f'results/lrt_class/MNIST/MNIST_KL_loss_FLOW_{HIDDEN_LAYERS}_hidden_{dim}_dim_{lr}_lr_{num_transforms}_num_trans' + '.txt', loss_several_runs, delimiter=',')\n",
    "    np.savetxt(f'results/lrt_class/MNIST/MNIST_KL_metrics_FLOW_{HIDDEN_LAYERS}_hidden_{dim}_dim_{lr}_lr_{num_transforms}_num_trans' + '.txt', metrics_several_runs, delimiter=',')\n",
    "    np.savetxt(f'results/lrt_class/MNIST/MNIST_KL_nll_FLOW_{HIDDEN_LAYERS}_hidden_{dim}_dim_{lr}_lr_{num_transforms}_num_trans' + '.txt', nll_several_runs, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc7f12-3634-44f1-847a-71a6e7b69752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c460ad-1b5b-41ec-89a6-71a3b9a36b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skip_con",
   "language": "python",
   "name": "skip_con"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
