{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "sys.path.append('networks')\n",
    "from flow_net import BayesianNetwork\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "The WBS dataset - classify breast cancer.\n",
    "\n",
    "# Batch size and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "a_prior = config['a_prior']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "\n",
    "X_original, y_original = load_breast_cancer(return_X_y=True)\n",
    "n, p = X_original.shape  # need this to get p \n",
    "\n",
    "print(n,p)\n",
    "\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int(n*0.8)\n",
    "TEST_BATCH_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int(n*0.80)\n",
    "TEST_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    copy.deepcopy(X_original), copy.deepcopy(y_original), test_size=0.10, random_state=42, stratify=y_original)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, a_prior=a_prior, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_sigmoid\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/prob/test0_sigmoid.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training:\n",
    "\n",
    "* 7 weights used in median model $\\rightarrow$ density of 0.7\\% compared to initialized model (960 weigths) \n",
    "* ACC of 92.8\\% for median model\n",
    "* ACC of 93.7\\% for full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weigth graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_sigmoid\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/weight/temp_sigmoid.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    copy.deepcopy(X_original), copy.deepcopy(y_original), test_size=0.10, random_state=42, stratify=y_original)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    #---------------------------\n",
    "    # DIFFERENCE IS IN act_func=F.relu part\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, a_prior=a_prior, num_transforms=num_transforms, act_func=F.relu).to(DEVICE)\n",
    "    #---------------------------\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_relu\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/prob/test0_relu.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training:\n",
    "\n",
    "* 8 weights used in median model $\\rightarrow$ density of 0.8\\% compared to initialized model (960 weigths) \n",
    "* ACC of 90.9\\% for median model\n",
    "* ACC of 91.9\\% for full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_relu\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/weight/temp_relu.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
