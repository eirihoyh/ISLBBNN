{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "import importlib\n",
    "importlib.reload(pf)\n",
    "importlib.reload(pip_func)\n",
    "sys.path.append('networks')\n",
    "from flow_sigmoid_net import BayesianNetwork\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attain data\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "## Pre process and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "# Get linear data, here a regression problem\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n*0.80)/100)\n",
    "TEST_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "\n",
    "## Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 564.9293823242188\n",
      "nll 17.257888793945312\n",
      "density 0.7314447390812414\n",
      "\n",
      "val_loss: 707.6798, val_nll: 158.2198, val_ensemble: 0.9895, used_weights_median: 38\n",
      "\n",
      "1\n",
      "loss 166.2592010498047\n",
      "nll 12.042278289794922\n",
      "density 0.25178228139325426\n",
      "\n",
      "val_loss: 284.4129, val_nll: 143.6056, val_ensemble: 0.9862, used_weights_median: 8\n",
      "\n",
      "2\n",
      "loss 79.34066772460938\n",
      "nll 14.564138412475586\n",
      "density 0.18172707091327067\n",
      "\n",
      "val_loss: 264.0968, val_nll: 205.6084, val_ensemble: 0.9760, used_weights_median: 8\n",
      "\n",
      "3\n",
      "loss 28.61278533935547\n",
      "nll 15.651078224182129\n",
      "density 0.1655927726102096\n",
      "\n",
      "val_loss: 382.7363, val_nll: 370.3861, val_ensemble: 0.9643, used_weights_median: 8\n",
      "\n",
      "4\n",
      "loss -0.7720432281494141\n",
      "nll 15.476224899291992\n",
      "density 0.13925996288243267\n",
      "\n",
      "val_loss: 97.7263, val_nll: 112.5806, val_ensemble: 0.9900, used_weights_median: 5\n",
      "\n",
      "5\n",
      "loss -21.119325637817383\n",
      "nll 27.09465980529785\n",
      "density 0.1253015967430892\n",
      "\n",
      "val_loss: 58.5322, val_nll: 117.6861, val_ensemble: 0.9868, used_weights_median: 2\n",
      "\n",
      "6\n",
      "loss -71.20240020751953\n",
      "nll 9.278299331665039\n",
      "density 0.12071025875802324\n",
      "\n",
      "val_loss: 125.7873, val_nll: 200.5110, val_ensemble: 0.9762, used_weights_median: 2\n",
      "\n",
      "7\n",
      "loss -78.19448852539062\n",
      "nll 8.88424301147461\n",
      "density 0.11794710737929025\n",
      "\n",
      "val_loss: 9.7087, val_nll: 79.3546, val_ensemble: 0.9935, used_weights_median: 2\n",
      "\n",
      "8\n",
      "loss -87.90080261230469\n",
      "nll 12.442614555358887\n",
      "density 0.11215702502576513\n",
      "\n",
      "val_loss: -7.7824, val_nll: 76.8485, val_ensemble: 0.9925, used_weights_median: 2\n",
      "\n",
      "9\n",
      "loss -52.482688903808594\n",
      "nll 52.45435333251953\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -40.3052, val_nll: 70.0353, val_ensemble: 0.9945, used_weights_median: 2\n",
      "\n",
      "10\n",
      "loss -116.42196655273438\n",
      "nll 7.051294326782227\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -15.7820, val_nll: 69.4260, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "11\n",
      "loss -103.0923843383789\n",
      "nll 19.41022491455078\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -93.3335, val_nll: 45.8268, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "12\n",
      "loss -138.8175048828125\n",
      "nll 10.089675903320312\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -93.0635, val_nll: 47.7126, val_ensemble: 0.9958, used_weights_median: 2\n",
      "\n",
      "13\n",
      "loss -153.01971435546875\n",
      "nll 4.706411361694336\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -108.1960, val_nll: 52.2087, val_ensemble: 0.9938, used_weights_median: 2\n",
      "\n",
      "14\n",
      "loss -162.4022674560547\n",
      "nll 4.301303863525391\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -84.2620, val_nll: 92.2067, val_ensemble: 0.9892, used_weights_median: 2\n",
      "\n",
      "15\n",
      "loss -169.4630126953125\n",
      "nll 14.808858871459961\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -125.1209, val_nll: 57.4376, val_ensemble: 0.9945, used_weights_median: 2\n",
      "\n",
      "16\n",
      "loss -183.21826171875\n",
      "nll 12.740888595581055\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -124.0714, val_nll: 65.1973, val_ensemble: 0.9922, used_weights_median: 2\n",
      "\n",
      "17\n",
      "loss -153.1051788330078\n",
      "nll 6.4371724128723145\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -139.3373, val_nll: 72.1644, val_ensemble: 0.9908, used_weights_median: 2\n",
      "\n",
      "18\n",
      "loss -199.4673309326172\n",
      "nll 3.039931535720825\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -162.0056, val_nll: 58.9705, val_ensemble: 0.9930, used_weights_median: 2\n",
      "\n",
      "19\n",
      "loss -235.34129333496094\n",
      "nll 4.625522613525391\n",
      "density 0.0827954940936319\n",
      "\n",
      "val_loss: -173.5954, val_nll: 60.4938, val_ensemble: 0.9925, used_weights_median: 2\n",
      "\n",
      "0.037037037 density median\n",
      "2.0 used weights median\n",
      "0.503925 ensemble full\n",
      "0.9920500000000001 ensemble median\n",
      "[0.503925, 0.037037037]\n"
     ]
    }
   ],
   "source": [
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00633232, 0.00682793, 0.0042301 , 0.00684217],\n",
       "        [0.00585916, 0.00680713, 0.00414927, 0.00638634],\n",
       "        [0.05251874, 0.18215206, 0.00663297, 0.00622487],\n",
       "        [0.00767064, 0.00753264, 0.00379462, 0.00553425],\n",
       "        [0.00664885, 0.00813247, 0.00496843, 0.00583838],\n",
       "        [0.00610813, 0.00765855, 0.00389322, 0.00615199],\n",
       "        [0.00552204, 0.00584669, 0.00163778, 0.00628702],\n",
       "        [0.00673197, 0.00750938, 0.00369952, 0.00653431],\n",
       "        [0.99996316, 0.9988244 , 0.00544006, 0.00563788],\n",
       "        [0.00622068, 0.00844101, 0.00341081, 0.00590927]], dtype=float32),\n",
       " array([[4.1774323e-04, 5.5158575e-04, 6.9025980e-04, 2.2072923e-04,\n",
       "         4.9270829e-04, 3.5440543e-04, 4.1501515e-04, 3.0016989e-04,\n",
       "         1.4086608e-02, 7.3907671e-05, 9.9999464e-01, 9.9999988e-01,\n",
       "         1.3635819e-03, 5.4842066e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.get_alphas_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.5592295 ,  0.44075483,  0.09219293,  0.85406816],\n",
       "        [ 0.35003656,  0.4219904 , -0.08232986,  0.14748089],\n",
       "        [-0.6021428 , -0.34066555,  0.27757537,  0.05496798],\n",
       "        [ 0.90761817,  1.0492636 , -0.02676811, -0.21698482],\n",
       "        [ 0.9758332 ,  0.8083283 , -0.04013633,  0.09636123],\n",
       "        [ 0.56553346,  0.46006137,  0.13256216,  0.17106597],\n",
       "        [ 0.48603112,  0.35061854,  0.19707102,  0.31749308],\n",
       "        [ 0.6637901 ,  0.5491373 , -0.10293567,  0.11483766],\n",
       "        [-0.00133723,  0.01921339,  0.01942176,  0.01242522],\n",
       "        [ 0.43062106,  0.47798884, -0.00203214, -0.00222193]],\n",
       "       dtype=float32),\n",
       " array([[ 4.6113085e-02, -3.1549659e-01,  1.8999761e-01,  2.5166939e-03,\n",
       "          2.6167503e-03,  8.3691822e-03,  1.2796061e-01,  3.6246911e-01,\n",
       "         -3.1050256e-01, -7.6871586e-04,  2.3277228e+00, -2.6576648e+00,\n",
       "         -1.0083407e-02, -9.1972956e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.weight_matrices_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
