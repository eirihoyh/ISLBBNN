{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "import importlib\n",
    "importlib.reload(pf)\n",
    "importlib.reload(pip_func)\n",
    "sys.path.append('networks')\n",
    "from flow_net import BayesianNetwork\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "# Batch size and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n_samples*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n_samples*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n_samples*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n_samples*0.80)/100)\n",
    "TEST_SIZE = int(n_samples*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n_samples*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n"
     ]
    }
   ],
   "source": [
    "# Get linear data\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 18.052453994750977\n",
      "nll 13.952274322509766\n",
      "density 0.06300279736104938\n",
      "\n",
      "val_loss: 238.5411, val_nll: 231.5988, val_ensemble: 0.9742, used_weights_median: 2\n",
      "\n",
      "1\n",
      "loss -59.035675048828125\n",
      "nll 12.017600059509277\n",
      "density 0.04594883611490433\n",
      "\n",
      "val_loss: -1.4761, val_nll: 76.1391, val_ensemble: 0.9925, used_weights_median: 2\n",
      "\n",
      "2\n",
      "loss -82.16539001464844\n",
      "nll 13.226521492004395\n",
      "density 0.04126501131991568\n",
      "\n",
      "val_loss: -15.2705, val_nll: 82.3173, val_ensemble: 0.9902, used_weights_median: 2\n",
      "\n",
      "3\n",
      "loss -97.15798950195312\n",
      "nll 11.435232162475586\n",
      "density 0.03937463191920398\n",
      "\n",
      "val_loss: -38.2766, val_nll: 51.7356, val_ensemble: 0.9940, used_weights_median: 2\n",
      "\n",
      "4\n",
      "loss -114.78987884521484\n",
      "nll 15.337733268737793\n",
      "density 0.038517852492511204\n",
      "\n",
      "val_loss: -38.1534, val_nll: 77.9296, val_ensemble: 0.9915, used_weights_median: 2\n",
      "\n",
      "5\n",
      "loss -108.51080322265625\n",
      "nll 17.130739212036133\n",
      "density 0.037987835522533694\n",
      "\n",
      "val_loss: -65.9236, val_nll: 79.6971, val_ensemble: 0.9905, used_weights_median: 2\n",
      "\n",
      "6\n",
      "loss -143.30099487304688\n",
      "nll 4.866847991943359\n",
      "density 0.03774502144921433\n",
      "\n",
      "val_loss: -85.8787, val_nll: 71.1453, val_ensemble: 0.9910, used_weights_median: 2\n",
      "\n",
      "7\n",
      "loss -150.31512451171875\n",
      "nll 13.255328178405762\n",
      "density 0.0376158665153815\n",
      "\n",
      "val_loss: 14.6070, val_nll: 147.3993, val_ensemble: 0.9852, used_weights_median: 2\n",
      "\n",
      "8\n",
      "loss -163.14080810546875\n",
      "nll 2.7776355743408203\n",
      "density 0.037533258900685366\n",
      "\n",
      "val_loss: -125.7771, val_nll: 39.1604, val_ensemble: 0.9948, used_weights_median: 2\n",
      "\n",
      "9\n",
      "loss -164.75013732910156\n",
      "nll 13.479987144470215\n",
      "density 0.03748399244405672\n",
      "\n",
      "val_loss: -103.5580, val_nll: 79.8967, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "10\n",
      "loss -182.1831817626953\n",
      "nll 2.7347140312194824\n",
      "density 0.0374520297038722\n",
      "\n",
      "val_loss: -158.4988, val_nll: 27.4521, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "11\n",
      "loss -126.19640350341797\n",
      "nll 14.262901306152344\n",
      "density 0.03742924414306318\n",
      "\n",
      "val_loss: -170.6291, val_nll: 27.7635, val_ensemble: 0.9970, used_weights_median: 2\n",
      "\n",
      "12\n",
      "loss -192.6404571533203\n",
      "nll 3.4245245456695557\n",
      "density 0.03741316444036364\n",
      "\n",
      "val_loss: -165.4860, val_nll: 35.4380, val_ensemble: 0.9948, used_weights_median: 2\n",
      "\n",
      "13\n",
      "loss -204.78659057617188\n",
      "nll 0.7331042885780334\n",
      "density 0.037402868481017974\n",
      "\n",
      "val_loss: -139.2086, val_nll: 67.0948, val_ensemble: 0.9925, used_weights_median: 2\n",
      "\n",
      "14\n",
      "loss -187.0387420654297\n",
      "nll 18.307722091674805\n",
      "density 0.037397018700418665\n",
      "\n",
      "val_loss: 54.9685, val_nll: 263.3430, val_ensemble: 0.9838, used_weights_median: 2\n",
      "\n",
      "15\n",
      "loss -201.925048828125\n",
      "nll 12.781538009643555\n",
      "density 0.037395945061910296\n",
      "\n",
      "val_loss: -118.6099, val_nll: 103.3741, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "16\n",
      "loss -197.5113067626953\n",
      "nll 31.825136184692383\n",
      "density 0.03739933903105638\n",
      "\n",
      "val_loss: -154.2764, val_nll: 54.4704, val_ensemble: 0.9935, used_weights_median: 2\n",
      "\n",
      "17\n",
      "loss -209.4259033203125\n",
      "nll 0.7782799601554871\n",
      "density 0.03740718590625874\n",
      "\n",
      "val_loss: -163.3645, val_nll: 51.2314, val_ensemble: 0.9938, used_weights_median: 2\n",
      "\n",
      "18\n",
      "loss -219.15933227539062\n",
      "nll 3.353480577468872\n",
      "density 0.037421652263985423\n",
      "\n",
      "val_loss: -108.5371, val_nll: 117.7216, val_ensemble: 0.9898, used_weights_median: 2\n",
      "\n",
      "19\n",
      "loss -231.76400756835938\n",
      "nll 1.4116833209991455\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: -113.6605, val_nll: 115.1711, val_ensemble: 0.9912, used_weights_median: 2\n",
      "\n",
      "20\n",
      "loss -257.73553466796875\n",
      "nll 3.5429913997650146\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: -215.9391, val_nll: 42.2927, val_ensemble: 0.9955, used_weights_median: 2\n",
      "\n",
      "21\n",
      "loss -259.4721374511719\n",
      "nll 14.20701789855957\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: 24.3860, val_nll: 293.9085, val_ensemble: 0.9822, used_weights_median: 2\n",
      "\n",
      "22\n",
      "loss -288.63677978515625\n",
      "nll 5.820525169372559\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: -250.1462, val_nll: 43.2947, val_ensemble: 0.9935, used_weights_median: 2\n",
      "\n",
      "23\n",
      "loss -284.00054931640625\n",
      "nll 16.76578140258789\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: -211.0871, val_nll: 99.0804, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "24\n",
      "loss -317.6352844238281\n",
      "nll 3.339433431625366\n",
      "density 0.037442942689551915\n",
      "\n",
      "val_loss: -305.2298, val_nll: 37.2888, val_ensemble: 0.9955, used_weights_median: 2\n",
      "\n",
      "0.037037037 density median\n",
      "2.0 used weights median\n",
      "0.791825 ensemble full\n",
      "0.996075 ensemble median\n",
      "[0.791825, 0.037037037]\n"
     ]
    }
   ],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_sigmoid\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/prob/test0_sigmoid.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training:\n",
    "\n",
    "* 2 weights used in median model $\\rightarrow$ density of 3.7% compared to initialized model (54 weigths) \n",
    "* ACC of 99\\% for median model\n",
    "* ACC of 80\\% for full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weigth graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_sigmoid\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/weight/temp_sigmoid.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n"
     ]
    }
   ],
   "source": [
    "# Get linear data\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 562.0380859375\n",
      "nll 11.622900009155273\n",
      "density 0.739109992980957\n",
      "\n",
      "val_loss: 763.2007, val_nll: 208.5790, val_ensemble: 0.9812, used_weights_median: 40\n",
      "\n",
      "1\n",
      "loss 177.28692626953125\n",
      "nll 9.171625137329102\n",
      "density 0.2751892829934756\n",
      "\n",
      "val_loss: 295.2562, val_nll: 143.8213, val_ensemble: 0.9832, used_weights_median: 8\n",
      "\n",
      "2\n",
      "loss 83.82557678222656\n",
      "nll 11.830016136169434\n",
      "density 0.19705630011028713\n",
      "\n",
      "val_loss: 246.4369, val_nll: 181.2499, val_ensemble: 0.9818, used_weights_median: 5\n",
      "\n",
      "3\n",
      "loss 37.5756721496582\n",
      "nll 8.783055305480957\n",
      "density 0.18293053882748442\n",
      "\n",
      "val_loss: 179.2229, val_nll: 145.0278, val_ensemble: 0.9872, used_weights_median: 5\n",
      "\n",
      "4\n",
      "loss 9.250143051147461\n",
      "nll 20.555009841918945\n",
      "density 0.14085748350178753\n",
      "\n",
      "val_loss: 169.6422, val_nll: 169.3849, val_ensemble: 0.9850, used_weights_median: 5\n",
      "\n",
      "5\n",
      "loss -14.245447158813477\n",
      "nll 14.790714263916016\n",
      "density 0.10892053656452508\n",
      "\n",
      "val_loss: 56.5653, val_nll: 80.6325, val_ensemble: 0.9920, used_weights_median: 5\n",
      "\n",
      "6\n",
      "loss -36.547119140625\n",
      "nll 12.06174373626709\n",
      "density 0.10228404593524626\n",
      "\n",
      "val_loss: 121.9438, val_nll: 168.9825, val_ensemble: 0.9860, used_weights_median: 5\n",
      "\n",
      "7\n",
      "loss -10.676168441772461\n",
      "nll 17.15617561340332\n",
      "density 0.09969869193661941\n",
      "\n",
      "val_loss: 227.4243, val_nll: 272.1036, val_ensemble: 0.9775, used_weights_median: 5\n",
      "\n",
      "8\n",
      "loss -50.753089904785156\n",
      "nll 7.129171371459961\n",
      "density 0.09874481727160013\n",
      "\n",
      "val_loss: 65.9731, val_nll: 111.8170, val_ensemble: 0.9880, used_weights_median: 5\n",
      "\n",
      "9\n",
      "loss -49.346351623535156\n",
      "nll 9.744260787963867\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: 88.3237, val_nll: 156.5109, val_ensemble: 0.9848, used_weights_median: 5\n",
      "\n",
      "10\n",
      "loss -64.33735656738281\n",
      "nll 6.706410884857178\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: 40.8350, val_nll: 99.9042, val_ensemble: 0.9908, used_weights_median: 5\n",
      "\n",
      "11\n",
      "loss -71.30397033691406\n",
      "nll 12.055436134338379\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -8.2076, val_nll: 45.3504, val_ensemble: 0.9952, used_weights_median: 5\n",
      "\n",
      "12\n",
      "loss -85.38853454589844\n",
      "nll 8.792865753173828\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -35.0413, val_nll: 48.5468, val_ensemble: 0.9948, used_weights_median: 5\n",
      "\n",
      "13\n",
      "loss -95.02017974853516\n",
      "nll 11.195243835449219\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: 3.3299, val_nll: 111.2827, val_ensemble: 0.9915, used_weights_median: 5\n",
      "\n",
      "14\n",
      "loss -92.72395324707031\n",
      "nll 11.057071685791016\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -1.5728, val_nll: 111.1443, val_ensemble: 0.9840, used_weights_median: 5\n",
      "\n",
      "15\n",
      "loss -100.5080337524414\n",
      "nll 15.564687728881836\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -53.2571, val_nll: 71.9673, val_ensemble: 0.9900, used_weights_median: 5\n",
      "\n",
      "16\n",
      "loss -76.33552551269531\n",
      "nll 41.16487503051758\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -12.5564, val_nll: 90.0910, val_ensemble: 0.9900, used_weights_median: 5\n",
      "\n",
      "17\n",
      "loss -132.032470703125\n",
      "nll 12.570127487182617\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -59.8477, val_nll: 84.8345, val_ensemble: 0.9905, used_weights_median: 5\n",
      "\n",
      "18\n",
      "loss -141.65234375\n",
      "nll 9.33358383178711\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -83.5473, val_nll: 73.5868, val_ensemble: 0.9895, used_weights_median: 5\n",
      "\n",
      "19\n",
      "loss -160.72459411621094\n",
      "nll 8.57744312286377\n",
      "density 0.09785858869643696\n",
      "\n",
      "val_loss: -66.5786, val_nll: 99.2363, val_ensemble: 0.9870, used_weights_median: 5\n",
      "\n",
      "0.09259259 density median\n",
      "5.0 used weights median\n",
      "0.49974999999999997 ensemble full\n",
      "0.986375 ensemble median\n",
      "[0.49974999999999997, 0.09259259]\n"
     ]
    }
   ],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    #---------------------------\n",
    "    # DIFFERENCE IS IN act_func=F.relu part\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms, act_func=F.relu).to(DEVICE)\n",
    "    #---------------------------\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_relu\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/prob/test0_relu.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training:\n",
    "\n",
    "* 5 weights used in median model $\\rightarrow$ density of 9.3% compared to initialized model (54 weigths) \n",
    "* ACC of 99\\% for median model\n",
    "* ACC of 50\\% for full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_relu\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"path_graphs/flow/weight/temp_relu.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
