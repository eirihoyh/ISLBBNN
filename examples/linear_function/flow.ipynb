{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "sys.path.append('networks')\n",
    "from flow_sigmoid_net import BayesianNetwork\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attain data\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "## Pre process and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "# Get linear data, here a regression problem\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n*0.80)/100)\n",
    "TEST_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "\n",
    "## Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 453.8760681152344\n",
      "nll 18.871662139892578\n",
      "density 0.73447059508827\n",
      "\n",
      "val_loss: 609.0934, val_nll: 170.7656, val_ensemble: 0.9928, used_weights_median: 38\n",
      "\n",
      "1\n",
      "loss 110.76991271972656\n",
      "nll 14.557331085205078\n",
      "density 0.24655262684380566\n",
      "\n",
      "val_loss: 264.5409, val_nll: 180.9322, val_ensemble: 0.9892, used_weights_median: 8\n",
      "\n",
      "2\n",
      "loss 27.08100700378418\n",
      "nll 13.355836868286133\n",
      "density 0.15052480423064143\n",
      "\n",
      "val_loss: 173.4721, val_nll: 166.9564, val_ensemble: 0.9872, used_weights_median: 2\n",
      "\n",
      "3\n",
      "loss -25.55683708190918\n",
      "nll 7.898965358734131\n",
      "density 0.11723341923896913\n",
      "\n",
      "val_loss: 96.5641, val_nll: 118.1691, val_ensemble: 0.9945, used_weights_median: 2\n",
      "\n",
      "4\n",
      "loss -43.99579620361328\n",
      "nll 8.606891632080078\n",
      "density 0.09364188144293924\n",
      "\n",
      "val_loss: 105.6388, val_nll: 160.1157, val_ensemble: 0.9812, used_weights_median: 2\n",
      "\n",
      "5\n",
      "loss -61.675724029541016\n",
      "nll 18.549068450927734\n",
      "density 0.08144892845733988\n",
      "\n",
      "val_loss: 24.1445, val_nll: 116.2475, val_ensemble: 0.9905, used_weights_median: 2\n",
      "\n",
      "6\n",
      "loss -99.64990234375\n",
      "nll 7.934062480926514\n",
      "density 0.05134496638223667\n",
      "\n",
      "val_loss: 53.6450, val_nll: 154.8014, val_ensemble: 0.9805, used_weights_median: 2\n",
      "\n",
      "7\n",
      "loss -94.31194305419922\n",
      "nll 13.266222953796387\n",
      "density 0.04639528445787383\n",
      "\n",
      "val_loss: 0.9487, val_nll: 94.1291, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "8\n",
      "loss -109.6212158203125\n",
      "nll 11.39178466796875\n",
      "density 0.0444829178644189\n",
      "\n",
      "val_loss: -19.1632, val_nll: 95.5528, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "9\n",
      "loss -108.68003845214844\n",
      "nll 8.91776180267334\n",
      "density 0.04292151319916431\n",
      "\n",
      "val_loss: -27.1712, val_nll: 101.7857, val_ensemble: 0.9905, used_weights_median: 2\n",
      "\n",
      "10\n",
      "loss -105.17958068847656\n",
      "nll 14.830567359924316\n",
      "density 0.04189639418976825\n",
      "\n",
      "val_loss: -14.4314, val_nll: 107.6840, val_ensemble: 0.9885, used_weights_median: 2\n",
      "\n",
      "11\n",
      "loss -126.29742431640625\n",
      "nll 6.734311580657959\n",
      "density 0.04152052476427815\n",
      "\n",
      "val_loss: -64.4347, val_nll: 71.4837, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "12\n",
      "loss -127.97701263427734\n",
      "nll 6.452584266662598\n",
      "density 0.040932848675732166\n",
      "\n",
      "val_loss: -53.4477, val_nll: 76.7068, val_ensemble: 0.9960, used_weights_median: 2\n",
      "\n",
      "13\n",
      "loss -134.21456909179688\n",
      "nll 7.859405994415283\n",
      "density 0.0401478399807847\n",
      "\n",
      "val_loss: -33.7940, val_nll: 108.8739, val_ensemble: 0.9875, used_weights_median: 2\n",
      "\n",
      "14\n",
      "loss -52.26542663574219\n",
      "nll 86.96112060546875\n",
      "density 0.03930354802997937\n",
      "\n",
      "val_loss: 10.4386, val_nll: 165.1662, val_ensemble: 0.9808, used_weights_median: 2\n",
      "\n",
      "15\n",
      "loss -141.27777099609375\n",
      "nll 10.060132026672363\n",
      "density 0.03883904132999305\n",
      "\n",
      "val_loss: -34.1743, val_nll: 113.6934, val_ensemble: 0.9868, used_weights_median: 2\n",
      "\n",
      "16\n",
      "loss -145.79071044921875\n",
      "nll 12.696866989135742\n",
      "density 0.03855612336586226\n",
      "\n",
      "val_loss: -14.1234, val_nll: 108.3474, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "17\n",
      "loss -148.27174377441406\n",
      "nll 8.573441505432129\n",
      "density 0.038395539814204044\n",
      "\n",
      "val_loss: -69.3364, val_nll: 81.7711, val_ensemble: 0.9942, used_weights_median: 2\n",
      "\n",
      "18\n",
      "loss -145.5748748779297\n",
      "nll 10.748915672302246\n",
      "density 0.03825769224426865\n",
      "\n",
      "val_loss: -54.9941, val_nll: 103.5134, val_ensemble: 0.9908, used_weights_median: 2\n",
      "\n",
      "19\n",
      "loss -157.86302185058594\n",
      "nll 8.874728202819824\n",
      "density 0.03814411348357933\n",
      "\n",
      "val_loss: -56.0902, val_nll: 109.0920, val_ensemble: 0.9885, used_weights_median: 2\n",
      "\n",
      "20\n",
      "loss -151.93309020996094\n",
      "nll 8.251577377319336\n",
      "density 0.03806505551419074\n",
      "\n",
      "val_loss: 57.4865, val_nll: 217.9978, val_ensemble: 0.9742, used_weights_median: 2\n",
      "\n",
      "21\n",
      "loss -154.6661834716797\n",
      "nll 6.503846168518066\n",
      "density 0.03801457697523313\n",
      "\n",
      "val_loss: 7.7937, val_nll: 171.0696, val_ensemble: 0.9792, used_weights_median: 2\n",
      "\n",
      "22\n",
      "loss -166.81158447265625\n",
      "nll 6.516488552093506\n",
      "density 0.03798589261040248\n",
      "\n",
      "val_loss: -72.4487, val_nll: 96.5030, val_ensemble: 0.9910, used_weights_median: 2\n",
      "\n",
      "23\n",
      "loss -159.95379638671875\n",
      "nll 7.374929904937744\n",
      "density 0.037961326766181164\n",
      "\n",
      "val_loss: -50.4964, val_nll: 111.2175, val_ensemble: 0.9885, used_weights_median: 2\n",
      "\n",
      "24\n",
      "loss -149.6468505859375\n",
      "nll 11.87210750579834\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -90.0207, val_nll: 92.2652, val_ensemble: 0.9905, used_weights_median: 2\n",
      "\n",
      "25\n",
      "loss -178.4825439453125\n",
      "nll 7.461841106414795\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -80.1069, val_nll: 108.4457, val_ensemble: 0.9875, used_weights_median: 2\n",
      "\n",
      "26\n",
      "loss -183.2581329345703\n",
      "nll 7.1502556800842285\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -112.3809, val_nll: 74.8918, val_ensemble: 0.9980, used_weights_median: 2\n",
      "\n",
      "27\n",
      "loss -187.7188720703125\n",
      "nll 6.7559380531311035\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -82.8452, val_nll: 110.9717, val_ensemble: 0.9902, used_weights_median: 2\n",
      "\n",
      "28\n",
      "loss -188.13818359375\n",
      "nll 9.028621673583984\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -102.4436, val_nll: 102.1116, val_ensemble: 0.9875, used_weights_median: 2\n",
      "\n",
      "29\n",
      "loss -191.5321044921875\n",
      "nll 11.362841606140137\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -74.3227, val_nll: 137.1445, val_ensemble: 0.9830, used_weights_median: 2\n",
      "\n",
      "30\n",
      "loss -199.99644470214844\n",
      "nll 7.506024360656738\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -138.7115, val_nll: 82.6693, val_ensemble: 0.9952, used_weights_median: 2\n",
      "\n",
      "31\n",
      "loss -223.92086791992188\n",
      "nll 8.511981010437012\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -108.7605, val_nll: 84.4485, val_ensemble: 0.9955, used_weights_median: 2\n",
      "\n",
      "32\n",
      "loss -196.10781860351562\n",
      "nll 7.088800430297852\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -97.4371, val_nll: 144.7697, val_ensemble: 0.9822, used_weights_median: 2\n",
      "\n",
      "33\n",
      "loss -224.26638793945312\n",
      "nll 7.329338550567627\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -142.0910, val_nll: 99.8287, val_ensemble: 0.9918, used_weights_median: 2\n",
      "\n",
      "34\n",
      "loss -241.78445434570312\n",
      "nll 8.833451271057129\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -153.5240, val_nll: 82.7559, val_ensemble: 0.9965, used_weights_median: 2\n",
      "\n",
      "35\n",
      "loss -232.29971313476562\n",
      "nll 9.569417953491211\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -51.9939, val_nll: 185.5123, val_ensemble: 0.9785, used_weights_median: 2\n",
      "\n",
      "36\n",
      "loss -260.21728515625\n",
      "nll 7.663978576660156\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -122.2762, val_nll: 145.1526, val_ensemble: 0.9842, used_weights_median: 2\n",
      "\n",
      "37\n",
      "loss -275.0740661621094\n",
      "nll 8.202985763549805\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -160.0421, val_nll: 121.1982, val_ensemble: 0.9858, used_weights_median: 2\n",
      "\n",
      "38\n",
      "loss -278.3730773925781\n",
      "nll 10.488055229187012\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -110.4679, val_nll: 173.0897, val_ensemble: 0.9812, used_weights_median: 2\n",
      "\n",
      "39\n",
      "loss -289.2027282714844\n",
      "nll 10.052716255187988\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -201.5283, val_nll: 104.4090, val_ensemble: 0.9908, used_weights_median: 2\n",
      "\n",
      "40\n",
      "loss -298.6535339355469\n",
      "nll 7.735350608825684\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -209.1838, val_nll: 104.9114, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "41\n",
      "loss -305.89495849609375\n",
      "nll 7.123207092285156\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -240.6219, val_nll: 81.9594, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "42\n",
      "loss -310.61785888671875\n",
      "nll 14.16417121887207\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -73.4370, val_nll: 258.7393, val_ensemble: 0.9700, used_weights_median: 2\n",
      "\n",
      "43\n",
      "loss -316.5094909667969\n",
      "nll 7.31173849105835\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -231.9049, val_nll: 109.5191, val_ensemble: 0.9895, used_weights_median: 2\n",
      "\n",
      "44\n",
      "loss -345.42431640625\n",
      "nll 14.23737907409668\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -236.5943, val_nll: 119.9837, val_ensemble: 0.9870, used_weights_median: 2\n",
      "\n",
      "45\n",
      "loss -348.45440673828125\n",
      "nll 8.04867935180664\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -257.8322, val_nll: 95.9443, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "46\n",
      "loss -332.9330749511719\n",
      "nll 9.60302734375\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -238.0723, val_nll: 106.0517, val_ensemble: 0.9890, used_weights_median: 2\n",
      "\n",
      "47\n",
      "loss -386.68255615234375\n",
      "nll 9.508610725402832\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -301.1845, val_nll: 90.8895, val_ensemble: 0.9930, used_weights_median: 2\n",
      "\n",
      "48\n",
      "loss -401.85333251953125\n",
      "nll 7.46292781829834\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -329.0763, val_nll: 77.2525, val_ensemble: 0.9998, used_weights_median: 2\n",
      "\n",
      "49\n",
      "loss -409.85205078125\n",
      "nll 8.194839477539062\n",
      "density 0.03796239958261188\n",
      "\n",
      "val_loss: -260.7531, val_nll: 163.3582, val_ensemble: 0.9818, used_weights_median: 2\n",
      "\n",
      "0.037037037 density median\n",
      "2.0 used weights median\n",
      "0.5059750000000001 ensemble full\n",
      "0.9874500000000002 ensemble median\n",
      "[0.5059750000000001, 0.037037037]\n"
     ]
    }
   ],
   "source": [
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7.87760109e-06, 5.86484530e-05, 1.44919886e-05, 5.82302595e-03],\n",
       "        [1.56587092e-04, 1.17364703e-04, 4.38525167e-05, 4.08909703e-03],\n",
       "        [8.84148176e-05, 4.00674879e-04, 3.64325788e-05, 4.68287617e-03],\n",
       "        [8.73189492e-05, 4.62921598e-05, 2.91525266e-05, 3.99240712e-03],\n",
       "        [2.55780215e-05, 9.29470843e-05, 2.99269177e-05, 3.89468390e-03],\n",
       "        [1.11794143e-04, 9.70581095e-05, 5.83579531e-05, 3.99460318e-03],\n",
       "        [4.54293286e-06, 3.44829532e-05, 1.42100125e-05, 4.20172932e-03],\n",
       "        [7.03233745e-05, 4.51400156e-05, 1.49041452e-05, 4.69373632e-03],\n",
       "        [9.17788639e-05, 7.55830297e-06, 1.10179717e-05, 5.06829424e-03],\n",
       "        [1.11250309e-04, 9.38020530e-05, 6.04475790e-05, 4.02996410e-03]],\n",
       "       dtype=float32),\n",
       " array([[2.6273625e-07, 7.3012620e-07, 5.5216583e-06, 2.7891547e-06,\n",
       "         7.3190307e-07, 2.1345820e-06, 2.8426817e-05, 2.7398286e-07,\n",
       "         2.4209069e-06, 6.6183979e-06, 9.9999928e-01, 1.0000000e+00,\n",
       "         1.0123221e-06, 3.3867238e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.get_alphas_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-9.99995787e-03,  2.72842357e-04,  2.57626530e-02,\n",
       "          4.77926172e-02],\n",
       "        [-3.70672010e-02,  4.95574763e-03,  5.47802337e-02,\n",
       "         -6.41293347e-01],\n",
       "        [-9.68932727e-05,  1.68750086e-03, -3.09986770e-02,\n",
       "         -1.33914024e-01],\n",
       "        [ 1.34886056e-03,  3.30697112e-02,  1.88033022e-02,\n",
       "         -5.24309158e-01],\n",
       "        [ 2.24700174e-03,  3.79590429e-02, -2.63027381e-02,\n",
       "         -3.37164104e-01],\n",
       "        [ 3.83165777e-02,  3.82795185e-02,  2.28392659e-03,\n",
       "         -1.11174941e-01],\n",
       "        [-6.39542788e-02,  2.53586303e-02,  1.40828174e-02,\n",
       "         -4.00992483e-01],\n",
       "        [ 2.74011381e-02, -1.80404400e-03, -3.49637866e-02,\n",
       "         -3.44291069e-02],\n",
       "        [ 1.03720136e-01,  9.47691206e-06,  5.51327467e-02,\n",
       "         -5.99362627e-02],\n",
       "        [-7.92662555e-04,  4.97676283e-02,  5.12579605e-02,\n",
       "         -8.50109696e-01]], dtype=float32),\n",
       " array([[-1.7587095e-04,  9.1777292e-06,  1.1637784e-03,  1.5023402e-05,\n",
       "          1.3170655e-04,  2.3475538e-04,  5.4298362e-05, -3.7722749e-04,\n",
       "         -5.0609575e-05, -3.9529614e-04,  1.2260071e+00, -1.3814545e+00,\n",
       "          1.0089431e-05,  8.4535358e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.weight_matrices_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
