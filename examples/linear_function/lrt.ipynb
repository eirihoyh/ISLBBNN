{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "sys.path.append('networks')\n",
    "from lrt_sigmoid_net import BayesianNetwork\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attain data\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "## Pre process and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "# Get linear data, here a regression problem\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n*0.80)/100)\n",
    "TEST_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "\n",
    "## Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 352.1375427246094\n",
      "nll 16.846179962158203\n",
      "density 0.7535007612572776\n",
      "\n",
      "val_loss: 605.5336, val_nll: 258.7996, val_ensemble: 0.9818, used_weights_median: 39\n",
      "\n",
      "1\n",
      "loss 80.17396545410156\n",
      "nll 21.203083038330078\n",
      "density 0.20620262129577221\n",
      "\n",
      "val_loss: 231.7242, val_nll: 174.6580, val_ensemble: 0.9882, used_weights_median: 7\n",
      "\n",
      "2\n",
      "loss 52.81830596923828\n",
      "nll 12.415382385253906\n",
      "density 0.13244875586005272\n",
      "\n",
      "val_loss: 163.1057, val_nll: 123.2184, val_ensemble: 0.9952, used_weights_median: 6\n",
      "\n",
      "3\n",
      "loss 49.68548583984375\n",
      "nll 9.130159378051758\n",
      "density 0.1271029103167907\n",
      "\n",
      "val_loss: 141.2609, val_nll: 100.6481, val_ensemble: 0.9985, used_weights_median: 6\n",
      "\n",
      "4\n",
      "loss 50.45975875854492\n",
      "nll 10.2677001953125\n",
      "density 0.12472027431552608\n",
      "\n",
      "val_loss: 133.8495, val_nll: 93.3276, val_ensemble: 0.9968, used_weights_median: 6\n",
      "\n",
      "5\n",
      "loss 46.809234619140625\n",
      "nll 11.675315856933594\n",
      "density 0.10539312931467537\n",
      "\n",
      "val_loss: 121.8381, val_nll: 86.3489, val_ensemble: 0.9970, used_weights_median: 5\n",
      "\n",
      "6\n",
      "loss 41.54051208496094\n",
      "nll 7.393409252166748\n",
      "density 0.10472034927699025\n",
      "\n",
      "val_loss: 116.2225, val_nll: 81.6294, val_ensemble: 0.9988, used_weights_median: 5\n",
      "\n",
      "7\n",
      "loss 41.907588958740234\n",
      "nll 7.838028907775879\n",
      "density 0.10463677177688589\n",
      "\n",
      "val_loss: 116.6811, val_nll: 82.0193, val_ensemble: 0.9980, used_weights_median: 5\n",
      "\n",
      "8\n",
      "loss 43.323951721191406\n",
      "nll 9.800298690795898\n",
      "density 0.10489881682830553\n",
      "\n",
      "val_loss: 114.2867, val_nll: 80.1011, val_ensemble: 0.9985, used_weights_median: 5\n",
      "\n",
      "9\n",
      "loss 40.31291580200195\n",
      "nll 6.778685092926025\n",
      "density 0.1054136101587641\n",
      "\n",
      "val_loss: 114.3271, val_nll: 80.1062, val_ensemble: 0.9980, used_weights_median: 5\n",
      "\n",
      "10\n",
      "loss 43.884010314941406\n",
      "nll 10.537954330444336\n",
      "density 0.10604238661902922\n",
      "\n",
      "val_loss: 115.4948, val_nll: 81.3080, val_ensemble: 0.9975, used_weights_median: 5\n",
      "\n",
      "11\n",
      "loss 40.946754455566406\n",
      "nll 9.302689552307129\n",
      "density 0.1052511936760749\n",
      "\n",
      "val_loss: 120.0663, val_nll: 86.8870, val_ensemble: 0.9962, used_weights_median: 5\n",
      "\n",
      "12\n",
      "loss 33.793270111083984\n",
      "nll 9.864201545715332\n",
      "density 0.08977714921692731\n",
      "\n",
      "val_loss: 155.7033, val_nll: 130.8000, val_ensemble: 0.9958, used_weights_median: 2\n",
      "\n",
      "13\n",
      "loss 31.075559616088867\n",
      "nll 8.050661087036133\n",
      "density 0.0906626081990975\n",
      "\n",
      "val_loss: 143.7145, val_nll: 119.6112, val_ensemble: 0.9972, used_weights_median: 2\n",
      "\n",
      "14\n",
      "loss 33.17155838012695\n",
      "nll 10.135210990905762\n",
      "density 0.09186680028128817\n",
      "\n",
      "val_loss: 136.7151, val_nll: 112.5473, val_ensemble: 0.9958, used_weights_median: 2\n",
      "\n",
      "15\n",
      "loss 31.64450454711914\n",
      "nll 8.728763580322266\n",
      "density 0.09323515817609236\n",
      "\n",
      "val_loss: 130.8551, val_nll: 106.7298, val_ensemble: 0.9965, used_weights_median: 2\n",
      "\n",
      "16\n",
      "loss 31.498371124267578\n",
      "nll 8.958344459533691\n",
      "density 0.09474912809673697\n",
      "\n",
      "val_loss: 125.2347, val_nll: 101.3477, val_ensemble: 0.9972, used_weights_median: 2\n",
      "\n",
      "17\n",
      "loss 31.840734481811523\n",
      "nll 9.395647048950195\n",
      "density 0.09650028509915703\n",
      "\n",
      "val_loss: 120.2179, val_nll: 96.3433, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "18\n",
      "loss 29.39815902709961\n",
      "nll 6.893657684326172\n",
      "density 0.09847714386544087\n",
      "\n",
      "val_loss: 117.0232, val_nll: 92.9455, val_ensemble: 0.9980, used_weights_median: 2\n",
      "\n",
      "19\n",
      "loss 29.79843521118164\n",
      "nll 7.033804893493652\n",
      "density 0.10059750007456858\n",
      "\n",
      "val_loss: 115.0840, val_nll: 90.6590, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "20\n",
      "loss 30.050371170043945\n",
      "nll 7.859737396240234\n",
      "density 0.102632236457861\n",
      "\n",
      "val_loss: 111.9281, val_nll: 87.9624, val_ensemble: 0.9975, used_weights_median: 2\n",
      "\n",
      "21\n",
      "loss 29.715469360351562\n",
      "nll 7.365097522735596\n",
      "density 0.10460263407461483\n",
      "\n",
      "val_loss: 109.7318, val_nll: 85.5640, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "22\n",
      "loss 28.396276473999023\n",
      "nll 6.173306941986084\n",
      "density 0.10658558616544017\n",
      "\n",
      "val_loss: 109.1394, val_nll: 84.9697, val_ensemble: 0.9970, used_weights_median: 2\n",
      "\n",
      "23\n",
      "loss 30.635440826416016\n",
      "nll 8.269031524658203\n",
      "density 0.10811605176422745\n",
      "\n",
      "val_loss: 107.3521, val_nll: 82.9573, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "24\n",
      "loss 29.73459243774414\n",
      "nll 7.53627347946167\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 106.0766, val_nll: 81.7831, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "25\n",
      "loss 29.358417510986328\n",
      "nll 7.171191692352295\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 103.0179, val_nll: 80.8299, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "26\n",
      "loss 29.924402236938477\n",
      "nll 7.748472690582275\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 102.0693, val_nll: 79.8832, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "27\n",
      "loss 29.32126235961914\n",
      "nll 7.264542579650879\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 100.0204, val_nll: 78.0032, val_ensemble: 0.9988, used_weights_median: 2\n",
      "\n",
      "28\n",
      "loss 28.789308547973633\n",
      "nll 6.6228461265563965\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 100.2540, val_nll: 78.0983, val_ensemble: 0.9965, used_weights_median: 2\n",
      "\n",
      "29\n",
      "loss 28.776859283447266\n",
      "nll 6.84548282623291\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 99.1440, val_nll: 77.2073, val_ensemble: 0.9972, used_weights_median: 2\n",
      "\n",
      "30\n",
      "loss 27.340011596679688\n",
      "nll 5.215278625488281\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 99.6954, val_nll: 77.6286, val_ensemble: 0.9975, used_weights_median: 2\n",
      "\n",
      "31\n",
      "loss 28.094575881958008\n",
      "nll 5.5209527015686035\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.5006, val_nll: 75.9237, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "32\n",
      "loss 30.064077377319336\n",
      "nll 7.624963283538818\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.7210, val_nll: 76.2928, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "33\n",
      "loss 27.85761070251465\n",
      "nll 5.445539951324463\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.3659, val_nll: 75.9743, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "34\n",
      "loss 27.613304138183594\n",
      "nll 6.072994232177734\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 96.9421, val_nll: 75.3810, val_ensemble: 0.9975, used_weights_median: 2\n",
      "\n",
      "35\n",
      "loss 28.16998291015625\n",
      "nll 5.631881237030029\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.4959, val_nll: 74.9598, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "36\n",
      "loss 27.7022705078125\n",
      "nll 5.26738977432251\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.6750, val_nll: 75.2713, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "37\n",
      "loss 29.151153564453125\n",
      "nll 6.909343719482422\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.6105, val_nll: 75.3885, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "38\n",
      "loss 30.66256332397461\n",
      "nll 8.847630500793457\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.4004, val_nll: 75.6029, val_ensemble: 0.9968, used_weights_median: 2\n",
      "\n",
      "39\n",
      "loss 28.34047508239746\n",
      "nll 5.815925598144531\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.5704, val_nll: 75.0537, val_ensemble: 0.9970, used_weights_median: 2\n",
      "\n",
      "40\n",
      "loss 28.747257232666016\n",
      "nll 6.590595245361328\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 96.9108, val_nll: 74.7901, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "41\n",
      "loss 27.932048797607422\n",
      "nll 5.1808648109436035\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.9016, val_nll: 75.1586, val_ensemble: 0.9968, used_weights_median: 2\n",
      "\n",
      "42\n",
      "loss 27.770891189575195\n",
      "nll 5.489425182342529\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.2265, val_nll: 74.9649, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "43\n",
      "loss 29.963665008544922\n",
      "nll 7.733212471008301\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 97.9223, val_nll: 75.6921, val_ensemble: 0.9978, used_weights_median: 2\n",
      "\n",
      "44\n",
      "loss 28.976058959960938\n",
      "nll 6.374251365661621\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 96.8869, val_nll: 74.3369, val_ensemble: 0.9982, used_weights_median: 2\n",
      "\n",
      "45\n",
      "loss 30.188007354736328\n",
      "nll 8.132067680358887\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.1336, val_nll: 76.1012, val_ensemble: 0.9975, used_weights_median: 2\n",
      "\n",
      "46\n",
      "loss 26.783294677734375\n",
      "nll 4.487768173217773\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.5061, val_nll: 76.2359, val_ensemble: 0.9975, used_weights_median: 2\n",
      "\n",
      "47\n",
      "loss 28.23446273803711\n",
      "nll 6.202727317810059\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 96.7585, val_nll: 74.7559, val_ensemble: 0.9985, used_weights_median: 2\n",
      "\n",
      "48\n",
      "loss 28.48259925842285\n",
      "nll 6.138730525970459\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.0133, val_nll: 75.6789, val_ensemble: 0.9972, used_weights_median: 2\n",
      "\n",
      "49\n",
      "loss 27.61119270324707\n",
      "nll 5.473310947418213\n",
      "density 0.10947008640613165\n",
      "\n",
      "val_loss: 98.2344, val_nll: 76.1456, val_ensemble: 0.9970, used_weights_median: 2\n",
      "\n",
      "0.037037037 density median\n",
      "2.0 used weights median\n",
      "0.9917750000000002 ensemble full\n",
      "0.9916499999999999 ensemble median\n",
      "[0.9917750000000002, 0.037037037]\n"
     ]
    }
   ],
   "source": [
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem))\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni], test_dat, DEVICE, SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/lrt/prob/test{ni}\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Seems like val_nll and val_loss is the same, but this is not true. Fix this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting weight magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/lrt/weight/temp\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.04988745, 0.04988742, 0.04988135, 0.02062172],\n",
       "        [0.04685967, 0.04959689, 0.04991695, 0.04983602],\n",
       "        [0.0498042 , 0.047339  , 0.04862013, 0.0498325 ],\n",
       "        [0.04926866, 0.04913945, 0.0495112 , 0.04981361],\n",
       "        [0.04986336, 0.02397774, 0.04915522, 0.04956369],\n",
       "        [0.9999999 , 0.99998546, 0.04150493, 0.02968235],\n",
       "        [0.04990653, 0.04990479, 0.04930333, 0.04955413],\n",
       "        [0.04664429, 0.0493351 , 0.04989627, 0.04982524],\n",
       "        [0.04990931, 0.04990511, 0.04989426, 0.04989322],\n",
       "        [0.01056166, 0.04988218, 0.04983991, 0.04980685]], dtype=float32),\n",
       " array([[1.7489504e-02, 1.7768953e-02, 8.5350974e-03, 1.8286632e-02,\n",
       "         1.5456454e-02, 9.6225744e-04, 1.7738851e-02, 1.7712986e-02,\n",
       "         1.7755657e-02, 1.7462607e-02, 1.0000000e+00, 9.9999976e-01,\n",
       "         2.3380416e-03, 2.2668489e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.get_alphas_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.3769492e-02,  1.2903831e-02,  4.7956183e-04,  3.1975642e-01],\n",
       "        [ 7.1882433e-01,  2.3529282e-01,  2.3785027e-02,  7.2391443e-02],\n",
       "        [ 1.1623137e-01,  5.7366705e-01,  7.0812022e-03, -1.7857222e-02],\n",
       "        [-1.5557915e-01, -8.0238491e-02,  2.4338402e-01, -9.3940124e-02],\n",
       "        [ 2.8213667e-02, -9.7034678e-02, -2.8946355e-01, -1.9545349e-01],\n",
       "        [-1.7739824e-08, -1.9720712e-08,  2.2152138e-03,  1.4615100e-03],\n",
       "        [-3.3941165e-02, -2.0993117e-02, -2.8660262e-01, -8.4736086e-02],\n",
       "        [-7.2170126e-01, -2.9008219e-01,  1.2678406e-02,  6.1826140e-02],\n",
       "        [ 1.2690320e-02,  4.7414652e-03, -5.2098912e-04,  2.2329235e-02],\n",
       "        [-9.9892294e-01,  3.0638261e-03, -5.8946375e-02,  6.2813371e-02]],\n",
       "       dtype=float32),\n",
       " array([[ 1.16950594e-01,  2.41647810e-01,  8.45124900e-01,\n",
       "          1.73156746e-02,  1.65198624e-01, -1.84700561e+00,\n",
       "         -7.16073960e-02, -1.26342952e-01,  6.96365163e-02,\n",
       "         -1.35228261e-01,  4.00830889e+00,  4.03862238e+00,\n",
       "         -1.70078280e-03,  1.39702915e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.weight_matrices_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
