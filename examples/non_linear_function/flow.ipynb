{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs are used!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "sys.path.append('networks')\n",
    "from flow_sigmoid_net import BayesianNetwork\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attain data\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + x_1\\cdot x_2 + x_1^2 + x_2^2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "## Pre process and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "# Get linear data, here a regression problem\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem, non_lin=non_lin)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n*0.80)/100)\n",
    "TEST_SIZE = int(n*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "\n",
    "## Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 642.6329956054688\n",
      "nll 102.93635559082031\n",
      "density 0.7308591364158524\n",
      "\n",
      "val_loss: 1514.6112, val_nll: 970.3116, val_ensemble: 0.9157, used_weights_median: 38\n",
      "\n",
      "1\n",
      "loss 280.2561340332031\n",
      "nll 67.78437805175781\n",
      "density 0.3440176741116577\n",
      "\n",
      "val_loss: 907.3023, val_nll: 716.1975, val_ensemble: 0.9360, used_weights_median: 12\n",
      "\n",
      "2\n",
      "loss 202.9445037841797\n",
      "nll 77.06227111816406\n",
      "density 0.2888615510116021\n",
      "\n",
      "val_loss: 894.8884, val_nll: 773.8152, val_ensemble: 0.9032, used_weights_median: 12\n",
      "\n",
      "3\n",
      "loss 234.55001831054688\n",
      "nll 136.05116271972656\n",
      "density 0.2760129857428924\n",
      "\n",
      "val_loss: 686.0006, val_nll: 583.4470, val_ensemble: 0.9307, used_weights_median: 12\n",
      "\n",
      "4\n",
      "loss 123.85497283935547\n",
      "nll 51.23870849609375\n",
      "density 0.2479298677623134\n",
      "\n",
      "val_loss: 807.9278, val_nll: 719.6821, val_ensemble: 0.9153, used_weights_median: 12\n",
      "\n",
      "5\n",
      "loss 98.47789001464844\n",
      "nll 37.98320770263672\n",
      "density 0.23213916607912527\n",
      "\n",
      "val_loss: 729.1021, val_nll: 676.9336, val_ensemble: 0.9137, used_weights_median: 12\n",
      "\n",
      "6\n",
      "loss 94.54295349121094\n",
      "nll 55.07236099243164\n",
      "density 0.22974593394341086\n",
      "\n",
      "val_loss: 728.9744, val_nll: 686.1699, val_ensemble: 0.9140, used_weights_median: 12\n",
      "\n",
      "7\n",
      "loss 82.45272827148438\n",
      "nll 44.20301055908203\n",
      "density 0.22795852817728668\n",
      "\n",
      "val_loss: 494.0391, val_nll: 436.0061, val_ensemble: 0.9587, used_weights_median: 12\n",
      "\n",
      "8\n",
      "loss 90.77426147460938\n",
      "nll 58.89042282104492\n",
      "density 0.2270778822825782\n",
      "\n",
      "val_loss: 623.9986, val_nll: 585.4059, val_ensemble: 0.9335, used_weights_median: 12\n",
      "\n",
      "9\n",
      "loss 63.74913787841797\n",
      "nll 36.635040283203125\n",
      "density 0.22638808356983914\n",
      "\n",
      "val_loss: 535.1095, val_nll: 514.2578, val_ensemble: 0.9400, used_weights_median: 12\n",
      "\n",
      "10\n",
      "loss 67.4628677368164\n",
      "nll 43.85118865966797\n",
      "density 0.22583216551720398\n",
      "\n",
      "val_loss: 497.8956, val_nll: 479.0884, val_ensemble: 0.9497, used_weights_median: 12\n",
      "\n",
      "11\n",
      "loss 55.364009857177734\n",
      "nll 37.469444274902344\n",
      "density 0.22530307336475947\n",
      "\n",
      "val_loss: 442.0969, val_nll: 427.9842, val_ensemble: 0.9493, used_weights_median: 12\n",
      "\n",
      "12\n",
      "loss 71.46814727783203\n",
      "nll 63.95172119140625\n",
      "density 0.22500770331882652\n",
      "\n",
      "val_loss: 509.6927, val_nll: 499.8856, val_ensemble: 0.9500, used_weights_median: 12\n",
      "\n",
      "13\n",
      "loss 45.855525970458984\n",
      "nll 44.24618148803711\n",
      "density 0.2246867233895197\n",
      "\n",
      "val_loss: 658.7258, val_nll: 654.1918, val_ensemble: 0.9327, used_weights_median: 12\n",
      "\n",
      "14\n",
      "loss 48.40941619873047\n",
      "nll 38.35962677001953\n",
      "density 0.2244098215533591\n",
      "\n",
      "val_loss: 342.8431, val_nll: 354.0519, val_ensemble: 0.9660, used_weights_median: 12\n",
      "\n",
      "15\n",
      "loss 28.00906753540039\n",
      "nll 29.477405548095703\n",
      "density 0.22423332329243648\n",
      "\n",
      "val_loss: 376.4862, val_nll: 383.2296, val_ensemble: 0.9593, used_weights_median: 12\n",
      "\n",
      "16\n",
      "loss 48.178401947021484\n",
      "nll 44.36172866821289\n",
      "density 0.22406359844005672\n",
      "\n",
      "val_loss: 444.7074, val_nll: 406.1337, val_ensemble: 0.9547, used_weights_median: 12\n",
      "\n",
      "17\n",
      "loss 22.354598999023438\n",
      "nll 31.776283264160156\n",
      "density 0.2238665893664823\n",
      "\n",
      "val_loss: 865.3295, val_nll: 866.3980, val_ensemble: 0.9075, used_weights_median: 12\n",
      "\n",
      "18\n",
      "loss 84.90400695800781\n",
      "nll 75.26717376708984\n",
      "density 0.22367863589419215\n",
      "\n",
      "val_loss: 420.0294, val_nll: 418.2841, val_ensemble: 0.9505, used_weights_median: 12\n",
      "\n",
      "19\n",
      "loss 33.00156021118164\n",
      "nll 39.9448127746582\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 539.6573, val_nll: 542.6292, val_ensemble: 0.9417, used_weights_median: 12\n",
      "\n",
      "20\n",
      "loss 26.719371795654297\n",
      "nll 39.463863372802734\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 786.3032, val_nll: 792.3492, val_ensemble: 0.9365, used_weights_median: 12\n",
      "\n",
      "21\n",
      "loss 18.53839111328125\n",
      "nll 37.64472198486328\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 559.7077, val_nll: 573.9984, val_ensemble: 0.9300, used_weights_median: 12\n",
      "\n",
      "22\n",
      "loss 9.486263275146484\n",
      "nll 34.059391021728516\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 494.6104, val_nll: 514.2300, val_ensemble: 0.9475, used_weights_median: 12\n",
      "\n",
      "23\n",
      "loss 15.522918701171875\n",
      "nll 33.27960968017578\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 502.2861, val_nll: 530.2840, val_ensemble: 0.9403, used_weights_median: 12\n",
      "\n",
      "24\n",
      "loss 23.163021087646484\n",
      "nll 29.357563018798828\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 310.3589, val_nll: 347.7623, val_ensemble: 0.9657, used_weights_median: 12\n",
      "\n",
      "25\n",
      "loss -5.911394119262695\n",
      "nll 25.706281661987305\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 297.4914, val_nll: 336.7954, val_ensemble: 0.9667, used_weights_median: 12\n",
      "\n",
      "26\n",
      "loss 1.9315376281738281\n",
      "nll 33.31059646606445\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 554.5032, val_nll: 588.6349, val_ensemble: 0.9313, used_weights_median: 12\n",
      "\n",
      "27\n",
      "loss 14.394805908203125\n",
      "nll 40.71454620361328\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 346.6710, val_nll: 360.5366, val_ensemble: 0.9673, used_weights_median: 12\n",
      "\n",
      "28\n",
      "loss 29.09143829345703\n",
      "nll 53.61338424682617\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 309.9385, val_nll: 350.0063, val_ensemble: 0.9685, used_weights_median: 12\n",
      "\n",
      "29\n",
      "loss 4.427753448486328\n",
      "nll 52.27251434326172\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 346.6555, val_nll: 377.4491, val_ensemble: 0.9605, used_weights_median: 12\n",
      "\n",
      "30\n",
      "loss 27.167835235595703\n",
      "nll 57.51606369018555\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 428.6460, val_nll: 483.3699, val_ensemble: 0.9405, used_weights_median: 12\n",
      "\n",
      "31\n",
      "loss -14.118396759033203\n",
      "nll 42.279964447021484\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 352.4496, val_nll: 402.4680, val_ensemble: 0.9613, used_weights_median: 12\n",
      "\n",
      "32\n",
      "loss 3.041492462158203\n",
      "nll 64.54237365722656\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 265.8477, val_nll: 332.8823, val_ensemble: 0.9665, used_weights_median: 12\n",
      "\n",
      "33\n",
      "loss -35.03849411010742\n",
      "nll 32.23967361450195\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 869.6747, val_nll: 920.0845, val_ensemble: 0.9130, used_weights_median: 12\n",
      "\n",
      "34\n",
      "loss 0.9510765075683594\n",
      "nll 60.332977294921875\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 443.4806, val_nll: 509.9549, val_ensemble: 0.9420, used_weights_median: 12\n",
      "\n",
      "35\n",
      "loss -21.918411254882812\n",
      "nll 51.894752502441406\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 372.4348, val_nll: 449.1043, val_ensemble: 0.9547, used_weights_median: 12\n",
      "\n",
      "36\n",
      "loss -0.9572830200195312\n",
      "nll 87.1712875366211\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 676.4812, val_nll: 765.0166, val_ensemble: 0.9340, used_weights_median: 12\n",
      "\n",
      "37\n",
      "loss -65.88339233398438\n",
      "nll 31.466751098632812\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 227.7825, val_nll: 321.7769, val_ensemble: 0.9692, used_weights_median: 12\n",
      "\n",
      "38\n",
      "loss -65.67333984375\n",
      "nll 30.908390045166016\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 246.1301, val_nll: 351.2797, val_ensemble: 0.9677, used_weights_median: 12\n",
      "\n",
      "39\n",
      "loss -29.59710693359375\n",
      "nll 54.75605773925781\n",
      "density 0.22358906224732883\n",
      "\n",
      "val_loss: 341.5363, val_nll: 457.1944, val_ensemble: 0.9590, used_weights_median: 12\n",
      "\n",
      "0.22222224 density median\n",
      "12.0 used weights median\n",
      "0.4976749999999999 ensemble full\n",
      "0.9602499999999999 ensemble median\n",
      "[0.4976749999999999, 0.22222224]\n"
     ]
    }
   ],
   "source": [
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[9.9999964e-01, 9.9996090e-01, 1.5115531e-04, 2.5657828e-03],\n",
       "        [2.6851173e-03, 2.9355574e-03, 1.4375076e-03, 2.6071442e-03],\n",
       "        [2.2750490e-03, 2.4024451e-03, 1.8528900e-03, 2.4174824e-03],\n",
       "        [2.9703800e-03, 2.4883435e-03, 2.2292605e-03, 2.1040537e-03],\n",
       "        [9.9999809e-01, 2.3150647e-03, 2.3487190e-04, 2.0145923e-03],\n",
       "        [2.5630447e-03, 3.0301993e-03, 1.4855900e-03, 2.6956520e-03],\n",
       "        [2.7008834e-03, 2.8946330e-03, 1.4668647e-03, 2.5904337e-03],\n",
       "        [2.6486372e-03, 3.2695218e-03, 1.3412469e-03, 2.6074641e-03],\n",
       "        [9.9997437e-01, 9.9998975e-01, 1.7341014e-04, 1.8760621e-03],\n",
       "        [2.5329755e-03, 9.9999666e-01, 1.6356248e-04, 2.1925431e-03]],\n",
       "       dtype=float32),\n",
       " array([[9.9999940e-01, 2.1541100e-05, 2.0293974e-05, 2.9557488e-05,\n",
       "         9.9999881e-01, 5.8485182e-07, 4.1964936e-06, 2.1145074e-06,\n",
       "         9.9999917e-01, 9.9999988e-01, 9.9999154e-01, 9.9999535e-01,\n",
       "         6.7507744e-06, 1.9013408e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.get_alphas_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 9.3055773e-01,  8.4735668e-01,  8.0786740e-05, -1.5375133e-02],\n",
       "        [ 1.4762272e-01,  7.9027258e-02,  6.3829917e-01,  6.4444321e-01],\n",
       "        [-3.2633621e-02, -7.1632033e-03, -7.2261584e-01, -1.0276437e-01],\n",
       "        [ 6.9746897e-02,  5.1625051e-02,  2.1614257e-02,  2.8378332e-01],\n",
       "        [ 7.6131731e-01,  1.6784146e-03,  3.4751797e-03,  9.1910101e-03],\n",
       "        [ 3.7883043e-01,  1.3904580e-01, -4.1000649e-01,  1.8288944e+00],\n",
       "        [ 2.3599084e-01,  1.2175232e-01,  3.8661119e-01,  6.2481534e-01],\n",
       "        [ 3.6576095e-03,  1.1774292e-02, -2.3552596e-03,  3.6518177e-01],\n",
       "        [-6.4198449e-02,  7.6945370e-01, -4.5933020e-03, -4.3603316e-02],\n",
       "        [ 7.9823658e-03,  7.3902744e-01, -9.6863206e-04,  7.8433268e-03]],\n",
       "       dtype=float32),\n",
       " array([[-4.37152052e+00,  1.74504844e-03, -2.56697871e-02,\n",
       "          2.92032049e-03,  4.54187250e+00,  6.65070684e-06,\n",
       "          8.74994993e-02,  8.64031899e-05,  2.78689027e+00,\n",
       "          2.36583805e+00, -8.49775434e-01, -3.79067004e-01,\n",
       "          1.18349035e-05, -3.40018742e-04]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_func.weight_matrices_numpy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
