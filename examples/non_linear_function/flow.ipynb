{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import config\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "path = \"C:\\\\Users\\\\eirik\\\\Documents\\\\Master\\\\ISLBBNN\\\\islbbnn\"\n",
    "# path = \"C:\\\\you\\\\path\\\\to\\\\islbbnn\\\\folder\\\\here\"\n",
    "os.chdir(path)\n",
    "import plot_functions as pf\n",
    "import pipeline_functions as pip_func\n",
    "sys.path.append('networks')\n",
    "from flow_net import BayesianNetwork\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.chdir(current_dir) # set the working directory back to this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "Problem:\n",
    "\n",
    "$$y = x_1 + x_2 + x_1\\cdot x_2 + x_1^2 + x_2^2 + 100 +\\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0,0.01)$. \n",
    "\n",
    "\n",
    "Can make $x_3$ dependent on $x_1$. The depedence is defined in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    " x_1 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &\\sim Unif(-10,10) \\\\\n",
    " x_3 &= \\text{dep}\\cdot x_1 + (1-\\text{dep})\\cdot x_3\n",
    "\\end{align*}\n",
    "\n",
    "# Batch size and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "HIDDEN_LAYERS = config['n_layers'] - 2 \n",
    "epochs = config['num_epochs']\n",
    "post_train_epochs = config['post_train_epochs']\n",
    "dim = config['hidden_dim']\n",
    "num_transforms = config['num_transforms']\n",
    "n_nets = config['n_nets']\n",
    "n_samples = config['n_samples']\n",
    "lr = config['lr']\n",
    "class_problem = config[\"class_problem\"]\n",
    "non_lin = config[\"non_lin\"]\n",
    "verbose = config['verbose']\n",
    "save_res = config['save_res']\n",
    "patience = config['patience']\n",
    "SAMPLES = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define BATCH sizes\n",
    "BATCH_SIZE = int((n_samples*0.8)/100)\n",
    "TEST_BATCH_SIZE = int(n_samples*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_BATCH_SIZE = int(n_samples*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "TRAIN_SIZE = int((n_samples*0.80)/100)\n",
    "TEST_SIZE = int(n_samples*0.10) # Would normally call this the \"validation\" part (will be used during training)\n",
    "VAL_SIZE = int(n_samples*0.10) # and this the \"test\" part (will be used after training)\n",
    "\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "\n",
    "print(NUM_BATCHES)\n",
    "\n",
    "assert (TRAIN_SIZE % BATCH_SIZE) == 0\n",
    "assert (TEST_SIZE % TEST_BATCH_SIZE) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n"
     ]
    }
   ],
   "source": [
    "# Get linear data\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem, non_lin=non_lin)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 562.8796997070312\n",
      "nll 14.278881072998047\n",
      "density 0.7347232878760055\n",
      "\n",
      "val_loss: 1065.6570, val_nll: 514.0360, val_ensemble: 0.9487, used_weights_median: 38\n",
      "\n",
      "1\n",
      "loss 175.22593688964844\n",
      "nll 9.803121566772461\n",
      "density 0.27324592391097985\n",
      "\n",
      "val_loss: 253.6071, val_nll: 103.0749, val_ensemble: 0.9988, used_weights_median: 11\n",
      "\n",
      "2\n",
      "loss 86.65093994140625\n",
      "nll 10.932417869567871\n",
      "density 0.21368368294228007\n",
      "\n",
      "val_loss: 331.6936, val_nll: 263.5049, val_ensemble: 0.9692, used_weights_median: 8\n",
      "\n",
      "3\n",
      "loss 31.768360137939453\n",
      "nll 8.54414176940918\n",
      "density 0.1866053121716336\n",
      "\n",
      "val_loss: 405.1871, val_nll: 373.1074, val_ensemble: 0.9680, used_weights_median: 8\n",
      "\n",
      "4\n",
      "loss -9.175753593444824\n",
      "nll 11.67617130279541\n",
      "density 0.11519127259789794\n",
      "\n",
      "val_loss: 200.1513, val_nll: 217.2041, val_ensemble: 0.9810, used_weights_median: 5\n",
      "\n",
      "5\n",
      "loss -17.528682708740234\n",
      "nll 19.59039306640625\n",
      "density 0.10166851057093246\n",
      "\n",
      "val_loss: 91.5460, val_nll: 136.2449, val_ensemble: 0.9842, used_weights_median: 5\n",
      "\n",
      "6\n",
      "loss -67.00894165039062\n",
      "nll 6.582812786102295\n",
      "density 0.08639184930749859\n",
      "\n",
      "val_loss: 52.1287, val_nll: 122.3060, val_ensemble: 0.9870, used_weights_median: 2\n",
      "\n",
      "7\n",
      "loss -67.2883529663086\n",
      "nll 16.215591430664062\n",
      "density 0.08216295859353893\n",
      "\n",
      "val_loss: 28.3487, val_nll: 103.2006, val_ensemble: 0.9892, used_weights_median: 2\n",
      "\n",
      "8\n",
      "loss -91.29149627685547\n",
      "nll 5.111695766448975\n",
      "density 0.08055621300418689\n",
      "\n",
      "val_loss: -6.5062, val_nll: 77.7183, val_ensemble: 0.9925, used_weights_median: 2\n",
      "\n",
      "9\n",
      "loss -91.78661346435547\n",
      "nll 5.1387128829956055\n",
      "density 0.07957735401578248\n",
      "\n",
      "val_loss: -35.3060, val_nll: 71.9823, val_ensemble: 0.9928, used_weights_median: 2\n",
      "\n",
      "10\n",
      "loss -75.39947509765625\n",
      "nll 17.710430145263672\n",
      "density 0.07891037314766436\n",
      "\n",
      "val_loss: 41.6367, val_nll: 144.2707, val_ensemble: 0.9830, used_weights_median: 2\n",
      "\n",
      "11\n",
      "loss -116.65660858154297\n",
      "nll 4.191209316253662\n",
      "density 0.07821398008474245\n",
      "\n",
      "val_loss: -43.8092, val_nll: 64.8212, val_ensemble: 0.9932, used_weights_median: 2\n",
      "\n",
      "12\n",
      "loss -87.6512680053711\n",
      "nll 34.04151153564453\n",
      "density 0.07777472767783264\n",
      "\n",
      "val_loss: -35.0814, val_nll: 76.2423, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "13\n",
      "loss -116.58023071289062\n",
      "nll 2.1648671627044678\n",
      "density 0.07746348272427929\n",
      "\n",
      "val_loss: -56.1418, val_nll: 70.0826, val_ensemble: 0.9920, used_weights_median: 2\n",
      "\n",
      "14\n",
      "loss -126.44567108154297\n",
      "nll 3.249211549758911\n",
      "density 0.07700503002708788\n",
      "\n",
      "val_loss: -60.9449, val_nll: 77.9114, val_ensemble: 0.9912, used_weights_median: 2\n",
      "\n",
      "15\n",
      "loss -128.3996124267578\n",
      "nll 5.0265793800354\n",
      "density 0.07677913860343913\n",
      "\n",
      "val_loss: -101.4510, val_nll: 43.4508, val_ensemble: 0.9970, used_weights_median: 2\n",
      "\n",
      "16\n",
      "loss -129.31590270996094\n",
      "nll 9.664246559143066\n",
      "density 0.07658508458548062\n",
      "\n",
      "val_loss: -88.2000, val_nll: 49.8137, val_ensemble: 0.9940, used_weights_median: 2\n",
      "\n",
      "17\n",
      "loss -130.44276428222656\n",
      "nll 8.751556396484375\n",
      "density 0.07637178376568989\n",
      "\n",
      "val_loss: -40.2852, val_nll: 101.3495, val_ensemble: 0.9885, used_weights_median: 2\n",
      "\n",
      "18\n",
      "loss -133.1600341796875\n",
      "nll 6.992560386657715\n",
      "density 0.07616443267372856\n",
      "\n",
      "val_loss: -86.7109, val_nll: 57.0750, val_ensemble: 0.9932, used_weights_median: 2\n",
      "\n",
      "19\n",
      "loss -127.88951110839844\n",
      "nll 8.392638206481934\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -81.1647, val_nll: 64.3268, val_ensemble: 0.9910, used_weights_median: 2\n",
      "\n",
      "20\n",
      "loss -153.77163696289062\n",
      "nll 2.686075210571289\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: 128.0175, val_nll: 278.4915, val_ensemble: 0.9780, used_weights_median: 2\n",
      "\n",
      "21\n",
      "loss -159.0542449951172\n",
      "nll 2.394927740097046\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -56.4738, val_nll: 89.4022, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "22\n",
      "loss -159.90646362304688\n",
      "nll 1.3402535915374756\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -110.0690, val_nll: 53.2377, val_ensemble: 0.9945, used_weights_median: 2\n",
      "\n",
      "23\n",
      "loss -161.35244750976562\n",
      "nll 5.208688735961914\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -69.7142, val_nll: 98.9704, val_ensemble: 0.9890, used_weights_median: 2\n",
      "\n",
      "24\n",
      "loss -143.78883361816406\n",
      "nll 10.018693923950195\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -105.7714, val_nll: 81.0334, val_ensemble: 0.9908, used_weights_median: 2\n",
      "\n",
      "25\n",
      "loss -180.351806640625\n",
      "nll 6.059813976287842\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -134.4628, val_nll: 56.0481, val_ensemble: 0.9932, used_weights_median: 2\n",
      "\n",
      "26\n",
      "loss -182.06057739257812\n",
      "nll 9.022723197937012\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -95.6587, val_nll: 91.8556, val_ensemble: 0.9898, used_weights_median: 2\n",
      "\n",
      "27\n",
      "loss -184.94149780273438\n",
      "nll 2.123255491256714\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -137.2474, val_nll: 51.1394, val_ensemble: 0.9945, used_weights_median: 2\n",
      "\n",
      "28\n",
      "loss -196.7852325439453\n",
      "nll 8.080204010009766\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -146.5074, val_nll: 57.1003, val_ensemble: 0.9922, used_weights_median: 2\n",
      "\n",
      "29\n",
      "loss -196.8003387451172\n",
      "nll 5.1922807693481445\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -154.2632, val_nll: 48.9376, val_ensemble: 0.9948, used_weights_median: 2\n",
      "\n",
      "30\n",
      "loss -209.49560546875\n",
      "nll 2.756117343902588\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -166.7784, val_nll: 33.4798, val_ensemble: 0.9965, used_weights_median: 2\n",
      "\n",
      "31\n",
      "loss -221.95468139648438\n",
      "nll 6.733354091644287\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -194.3643, val_nll: 28.3759, val_ensemble: 0.9990, used_weights_median: 2\n",
      "\n",
      "32\n",
      "loss -221.29954528808594\n",
      "nll 1.4734411239624023\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -131.2959, val_nll: 105.4856, val_ensemble: 0.9890, used_weights_median: 2\n",
      "\n",
      "33\n",
      "loss -223.9342041015625\n",
      "nll 10.944671630859375\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -193.8754, val_nll: 28.3572, val_ensemble: 0.9992, used_weights_median: 2\n",
      "\n",
      "34\n",
      "loss -238.12632751464844\n",
      "nll 7.699306964874268\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -166.4697, val_nll: 74.4470, val_ensemble: 0.9902, used_weights_median: 2\n",
      "\n",
      "35\n",
      "loss -253.2593231201172\n",
      "nll 2.7014989852905273\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -168.4503, val_nll: 84.4948, val_ensemble: 0.9912, used_weights_median: 2\n",
      "\n",
      "36\n",
      "loss -262.4769287109375\n",
      "nll 2.3993217945098877\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -151.1948, val_nll: 112.5710, val_ensemble: 0.9885, used_weights_median: 2\n",
      "\n",
      "37\n",
      "loss -275.0218811035156\n",
      "nll 1.9964338541030884\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -198.1510, val_nll: 78.2005, val_ensemble: 0.9908, used_weights_median: 2\n",
      "\n",
      "38\n",
      "loss -268.8517761230469\n",
      "nll 5.3469343185424805\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -188.8694, val_nll: 95.3910, val_ensemble: 0.9900, used_weights_median: 2\n",
      "\n",
      "39\n",
      "loss -291.01031494140625\n",
      "nll 1.142153263092041\n",
      "density 0.07605307264777315\n",
      "\n",
      "val_loss: -236.6435, val_nll: 57.9974, val_ensemble: 0.9935, used_weights_median: 2\n",
      "\n",
      "0.037037037 density median\n",
      "2.0 used weights median\n",
      "0.51625 ensemble full\n",
      "0.9957750000000001 ensemble median\n",
      "[0.51625, 0.037037037]\n"
     ]
    }
   ],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms).to(DEVICE)\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_sigmoid\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weigth graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_sigmoid\", show=True, flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU based network\n",
    "\n",
    "## Seperate a test set for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4 10\n"
     ]
    }
   ],
   "source": [
    "# Get linear data\n",
    "y, X = pip_func.create_data_unif(n_samples, beta=[100,1,1,1,1], dep_level=0.0, classification=class_problem, non_lin=non_lin)\n",
    "\n",
    "n, p = X.shape  # need this to get p \n",
    "print(n,p,dim)\n",
    "\n",
    "# Split keep some of the data for validation after training\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)#, stratify=y)\n",
    "\n",
    "test_dat = torch.tensor(np.column_stack((X_test,y_test)),dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, and test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network 0\n",
      "54\n",
      "0\n",
      "loss 629.3342895507812\n",
      "nll 87.3471450805664\n",
      "density 0.7258114701619854\n",
      "\n",
      "val_loss: 1408.0122, val_nll: 861.5485, val_ensemble: 0.9353, used_weights_median: 38\n",
      "\n",
      "1\n",
      "loss 282.1619873046875\n",
      "nll 72.46800231933594\n",
      "density 0.3412576043219478\n",
      "\n",
      "val_loss: 890.9567, val_nll: 702.3647, val_ensemble: 0.9455, used_weights_median: 12\n",
      "\n",
      "2\n",
      "loss 178.39974975585938\n",
      "nll 65.40678405761719\n",
      "density 0.26799427497166173\n",
      "\n",
      "val_loss: 692.7062, val_nll: 584.8984, val_ensemble: 0.9513, used_weights_median: 12\n",
      "\n",
      "3\n",
      "loss 153.10549926757812\n",
      "nll 80.31148529052734\n",
      "density 0.2226462967314378\n",
      "\n",
      "val_loss: 724.8666, val_nll: 636.7312, val_ensemble: 0.9323, used_weights_median: 10\n",
      "\n",
      "4\n",
      "loss 132.9916534423828\n",
      "nll 79.62358856201172\n",
      "density 0.21602045356606445\n",
      "\n",
      "val_loss: 639.2435, val_nll: 574.3133, val_ensemble: 0.9453, used_weights_median: 10\n",
      "\n",
      "5\n",
      "loss 94.40233612060547\n",
      "nll 53.7294921875\n",
      "density 0.2125562926543083\n",
      "\n",
      "val_loss: 593.3013, val_nll: 561.6805, val_ensemble: 0.9450, used_weights_median: 10\n",
      "\n",
      "6\n",
      "loss 84.23247528076172\n",
      "nll 61.388832092285156\n",
      "density 0.2105721566115756\n",
      "\n",
      "val_loss: 581.9346, val_nll: 558.7831, val_ensemble: 0.9390, used_weights_median: 10\n",
      "\n",
      "7\n",
      "loss 97.74024200439453\n",
      "nll 52.95256042480469\n",
      "density 0.2088998912941962\n",
      "\n",
      "val_loss: 987.2466, val_nll: 956.8872, val_ensemble: 0.8712, used_weights_median: 10\n",
      "\n",
      "8\n",
      "loss 69.57520294189453\n",
      "nll 50.91812515258789\n",
      "density 0.20814708285831363\n",
      "\n",
      "val_loss: 500.8495, val_nll: 475.6167, val_ensemble: 0.9507, used_weights_median: 10\n",
      "\n",
      "9\n",
      "loss 61.790016174316406\n",
      "nll 46.7579345703125\n",
      "density 0.20760620793576148\n",
      "\n",
      "val_loss: 681.6722, val_nll: 673.1162, val_ensemble: 0.9325, used_weights_median: 10\n",
      "\n",
      "10\n",
      "loss 65.24055480957031\n",
      "nll 49.1456298828125\n",
      "density 0.18933657210763394\n",
      "\n",
      "val_loss: 449.1744, val_nll: 445.4418, val_ensemble: 0.9613, used_weights_median: 10\n",
      "\n",
      "11\n",
      "loss 96.29959106445312\n",
      "nll 86.86065673828125\n",
      "density 0.18838713433372808\n",
      "\n",
      "val_loss: 493.4575, val_nll: 484.0038, val_ensemble: 0.9477, used_weights_median: 10\n",
      "\n",
      "12\n",
      "loss 54.0842399597168\n",
      "nll 59.66555404663086\n",
      "density 0.18802177847427864\n",
      "\n",
      "val_loss: 539.6542, val_nll: 545.4236, val_ensemble: 0.9385, used_weights_median: 10\n",
      "\n",
      "13\n",
      "loss 36.712928771972656\n",
      "nll 47.40227508544922\n",
      "density 0.1877282501384259\n",
      "\n",
      "val_loss: 636.3487, val_nll: 648.2217, val_ensemble: 0.9235, used_weights_median: 10\n",
      "\n",
      "14\n",
      "loss 43.8574104309082\n",
      "nll 55.69660568237305\n",
      "density 0.18740022998071867\n",
      "\n",
      "val_loss: 471.9596, val_nll: 492.8657, val_ensemble: 0.9440, used_weights_median: 10\n",
      "\n",
      "15\n",
      "loss 25.1639404296875\n",
      "nll 34.30308532714844\n",
      "density 0.18724443004938227\n",
      "\n",
      "val_loss: 379.0055, val_nll: 399.5944, val_ensemble: 0.9550, used_weights_median: 10\n",
      "\n",
      "16\n",
      "loss 15.910850524902344\n",
      "nll 44.572425842285156\n",
      "density 0.187013001711967\n",
      "\n",
      "val_loss: 549.7081, val_nll: 533.2510, val_ensemble: 0.9380, used_weights_median: 10\n",
      "\n",
      "17\n",
      "loss 23.575130462646484\n",
      "nll 43.7160758972168\n",
      "density 0.18688727691983595\n",
      "\n",
      "val_loss: 459.3568, val_nll: 473.7528, val_ensemble: 0.9433, used_weights_median: 10\n",
      "\n",
      "18\n",
      "loss 23.349952697753906\n",
      "nll 46.09840393066406\n",
      "density 0.18676045574541855\n",
      "\n",
      "val_loss: 564.5859, val_nll: 593.6179, val_ensemble: 0.9343, used_weights_median: 10\n",
      "\n",
      "19\n",
      "loss 14.409656524658203\n",
      "nll 39.988529205322266\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 559.3176, val_nll: 584.7917, val_ensemble: 0.9315, used_weights_median: 10\n",
      "\n",
      "20\n",
      "loss -1.5439834594726562\n",
      "nll 39.45414733886719\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 448.0295, val_nll: 482.5205, val_ensemble: 0.9513, used_weights_median: 10\n",
      "\n",
      "21\n",
      "loss -8.804893493652344\n",
      "nll 37.64087677001953\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 478.0197, val_nll: 521.3600, val_ensemble: 0.9377, used_weights_median: 10\n",
      "\n",
      "22\n",
      "loss -3.7201004028320312\n",
      "nll 41.712093353271484\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 509.6883, val_nll: 549.7605, val_ensemble: 0.9370, used_weights_median: 10\n",
      "\n",
      "23\n",
      "loss -16.62732696533203\n",
      "nll 41.20801544189453\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 467.7990, val_nll: 526.3961, val_ensemble: 0.9393, used_weights_median: 10\n",
      "\n",
      "24\n",
      "loss -21.119491577148438\n",
      "nll 41.57136535644531\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 301.1436, val_nll: 371.8322, val_ensemble: 0.9643, used_weights_median: 10\n",
      "\n",
      "25\n",
      "loss -38.39680480957031\n",
      "nll 36.25654602050781\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 386.3994, val_nll: 452.2081, val_ensemble: 0.9467, used_weights_median: 10\n",
      "\n",
      "26\n",
      "loss -35.5313720703125\n",
      "nll 40.171966552734375\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 312.4305, val_nll: 386.3845, val_ensemble: 0.9553, used_weights_median: 10\n",
      "\n",
      "27\n",
      "loss -42.65254592895508\n",
      "nll 40.15250778198242\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 457.8039, val_nll: 528.9811, val_ensemble: 0.9347, used_weights_median: 10\n",
      "\n",
      "28\n",
      "loss -20.526229858398438\n",
      "nll 41.62832260131836\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 394.5869, val_nll: 478.9148, val_ensemble: 0.9463, used_weights_median: 10\n",
      "\n",
      "29\n",
      "loss -56.061248779296875\n",
      "nll 34.30924987792969\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 283.4691, val_nll: 371.7608, val_ensemble: 0.9627, used_weights_median: 10\n",
      "\n",
      "30\n",
      "loss -54.87509536743164\n",
      "nll 39.238224029541016\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 288.2023, val_nll: 398.7414, val_ensemble: 0.9567, used_weights_median: 10\n",
      "\n",
      "31\n",
      "loss -83.301025390625\n",
      "nll 40.6773567199707\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 368.2004, val_nll: 481.6355, val_ensemble: 0.9425, used_weights_median: 10\n",
      "\n",
      "32\n",
      "loss -81.48355865478516\n",
      "nll 44.33855438232422\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 602.8143, val_nll: 728.5562, val_ensemble: 0.9215, used_weights_median: 10\n",
      "\n",
      "33\n",
      "loss -93.18075561523438\n",
      "nll 34.84315490722656\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 537.0818, val_nll: 658.9827, val_ensemble: 0.9225, used_weights_median: 10\n",
      "\n",
      "34\n",
      "loss -87.339599609375\n",
      "nll 35.695316314697266\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 318.7390, val_nll: 464.3165, val_ensemble: 0.9465, used_weights_median: 10\n",
      "\n",
      "35\n",
      "loss -98.40098571777344\n",
      "nll 52.3372688293457\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 345.6499, val_nll: 452.1814, val_ensemble: 0.9513, used_weights_median: 10\n",
      "\n",
      "36\n",
      "loss -120.12629699707031\n",
      "nll 36.80644607543945\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 236.9769, val_nll: 383.8735, val_ensemble: 0.9560, used_weights_median: 10\n",
      "\n",
      "37\n",
      "loss -123.09696960449219\n",
      "nll 49.51268005371094\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 324.4889, val_nll: 494.4996, val_ensemble: 0.9417, used_weights_median: 10\n",
      "\n",
      "38\n",
      "loss -130.80218505859375\n",
      "nll 33.22517395019531\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 413.7456, val_nll: 601.7240, val_ensemble: 0.9415, used_weights_median: 10\n",
      "\n",
      "39\n",
      "loss -145.54942321777344\n",
      "nll 29.634794235229492\n",
      "density 0.1865521490265449\n",
      "\n",
      "val_loss: 103.9517, val_nll: 292.3911, val_ensemble: 0.9712, used_weights_median: 10\n",
      "\n",
      "0.18518518 density median\n",
      "10.0 used weights median\n",
      "0.5020999999999999 ensemble full\n",
      "0.9609249999999999 ensemble median\n",
      "[0.5020999999999999, 0.18518518]\n"
     ]
    }
   ],
   "source": [
    "# select the device and initiate model\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "all_nets = {}\n",
    "metrics_several_runs = []\n",
    "metrics_median_several_runs = []\n",
    "for ni in range(n_nets):\n",
    "    post_train = False\n",
    "    print('network', ni)\n",
    "    # Initate network\n",
    "    torch.manual_seed(ni+42)\n",
    "    #---------------------------\n",
    "    # DIFFERENCE IS IN act_func=F.relu part\n",
    "    net = BayesianNetwork(dim, p, HIDDEN_LAYERS, classification=class_problem, num_transforms=num_transforms, act_func=F.relu).to(DEVICE)\n",
    "    #---------------------------\n",
    "    alphas = pip_func.get_alphas_numpy(net)\n",
    "    nr_weights = np.sum([np.prod(a.shape) for a in alphas])\n",
    "    print(nr_weights)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    all_nll = []\n",
    "    all_loss = []\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=1/9, random_state=ni)#, stratify=y)\n",
    "            \n",
    "    train_dat = torch.tensor(np.column_stack((X_train,y_train)),dtype = torch.float32)\n",
    "    val_dat = torch.tensor(np.column_stack((X_val,y_val)),dtype = torch.float32)\n",
    "    \n",
    "    # Train network\n",
    "    counter = 0\n",
    "    highest_acc = 0\n",
    "    best_model = copy.deepcopy(net)\n",
    "    for epoch in range(epochs + post_train_epochs):\n",
    "        if verbose:\n",
    "            print(epoch)\n",
    "        nll, loss = pip_func.train(net, train_dat, optimizer, BATCH_SIZE, NUM_BATCHES, p, DEVICE, nr_weights, post_train=post_train)\n",
    "        nll_val, loss_val, ensemble_val = pip_func.val(net, val_dat, DEVICE, verbose=verbose, reg=(not class_problem), post_train=post_train)\n",
    "        if ensemble_val >= highest_acc:\n",
    "            counter = 0\n",
    "            highest_acc = ensemble_val\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        all_nll.append(nll)\n",
    "        all_loss.append(loss)\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            post_train = True   # Post-train --> use median model \n",
    "            for name, param in net.named_parameters():\n",
    "                for i in range(HIDDEN_LAYERS+1):\n",
    "                    #if f\"linears{i}.lambdal\" in name:\n",
    "                    if f\"linears.{i}.lambdal\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "        \n",
    "    all_nets[ni] = net \n",
    "    # Results\n",
    "    metrics, metrics_median = pip_func.test_ensemble(all_nets[ni],test_dat,DEVICE,SAMPLES=10, reg=(not class_problem)) # Test same data 10 times to get average \n",
    "    metrics_several_runs.append(metrics)\n",
    "    metrics_median_several_runs.append(metrics_median)\n",
    "    pf.run_path_graph(all_nets[ni], threshold=0.5, save_path=f\"path_graphs/flow/prob/test{ni}_relu\", show=verbose)\n",
    "\n",
    "if verbose:\n",
    "    print(metrics)\n",
    "m = np.array(metrics_several_runs)\n",
    "m_median = np.array(metrics_median_several_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attain weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.run_path_graph_weight(net, save_path=\"path_graphs/flow/weight/temp_relu\", show=True, flow=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
